This thesis presents the partially stateful model, as well as its implementation
in Noria. And while the model is complete in isolation, there are a number of
secondary considerations, features, and alternatives that are worth discussing.
Those are discussed in this chapter.

\begin{inprogress}
Some of this should maybe be future work?
\end{inprogress}

\begin{inprogress}
Should some of these go under "Noria background"?
\end{inprogress}

\section{When is Noria not the Answer?}

Noria aims to improve the efficiency of certain classes of database-backed
applications, but is not a one-size-fits-all solution. Noria's materialized
views generally, and partial state specifically, are built specifically for
applications that:

\begin{enumerate}
  \item Are \textbf{read-heavy}. Noria's design is centered around making reads
    cheap, often at the cost of write performance. For workloads where writes
    are as frequent, or more frequent, than reads, other systems will work
    better.
  \item Tolerate \textbf{eventual consistency}, at least for large parts of the
    application's workload. Much of Noria's performance advantages over other
    systems stems from the relaxed consistency model. If much of the
    application's workload requires stronger consistency guarantees, there is
    little for Noria to speed up.
  \item Experience \textbf{good locality}. If the application's access patterns
    are all completely uniform, caching is much less impactful unless \emph{all}
    results are cached. In that case, partial state, and the complexity it
    introduces, provides little value. Instead, Noria works best if data and
    access distributions are skewed, and demonstrate good temporal and spatial
    locality.
  \item Have \textbf{non-trivial computed state}, both in size and complexity.
    If all computed state fits in a small amount of memory, a materialized view
    system without partial state would work just as well. If all queries are
    simple point queries without aggregations or joins, Noria's incremental
    cache update logic is unnecessary, and a simpler cache invalidation scheme
    may work better.
\end{enumerate}

It is also worth noting that Noria may not perform as well as a fully developed,
manually tuned backend caching system. While Noria would allow the removal of
caching logic from the application, its general-purpose architecture may miss
out on application-specific optimizations implemented by a tailor-built system.

\section{Efficient Migrations}

Section~\ref{s:eval:mig} demonstrated that partial state can make certain
migrations efficient. This requires that the view \emph{can} be partial, as
per the discussion above. But even for views that can be partial, work may be
required in order to make upqueries for that view efficient. This work generally
means adding an index to some existing state, which requires scanning the data
stored in that view. Constructing an index tends to be significantly faster than
computing the full cached results of the new view, but it is a non-trivial cost
nonetheless.

For example, consider the query in Listing~\vref{l:karma} when added to the vote
benchmark from Listing~\vref{l:votes}. To simplify the argument, assume that the
\texttt{VoteCount} view is for some reason \emph{not} partially stateful (i.e.,
it holds all the rows). For upqueries of the new view to be efficient, it must
be possible to query all the articles (along with their vote counts) for a given
author in the \texttt{ViewCount} view that existed previously. This means we
must add an index on the \texttt{author} column of that view's state, which is
costly.

\begin{listing}[h]
  \begin{minted}{sql}
    SELECT VoteCount.author, SUM(VoteCount.nvotes) AS karma
    FROM VoteCount -- the view from the vote benchmark
    GROUP BY VoteCount.author
    WHERE VoteCount.author = ?
  \end{minted}
  \caption{Query that computes the sum total score of a user's articles
  (their ``karma'').}
  \label{l:karma}
\end{listing}

\begin{inprogress}
  If \texttt{VoteCount} is partial, it \emph{is} free, because indexes for
  partially stateful views always start out empty. Each index of a partially
  stateful view is completely independent! There are no "shared" rows across
  indexes. But we probably want there to be.
\end{inprogress}

A comparison with what would happen when using a traditional relational database
is useful here. When the application developer decides that they want to run
this new query, they have two choices: either compute it on-demand, or
denormalize the schema by adding a new computed ``karma'' column to the
(hypothesized) \texttt{users} table. Neither option is great. The former is slow
to execute, and the latter requires computing the karma for every article.
The index Noria must construct for efficient upqueries is cheaper to construct
than such a computed \texttt{karma} column, which makes Noria's scan seems
reasonable.

% SELECT COUNT(*) FROM table;

Whether Noria \emph{always} does no more work than what a developer would
make a traditional relational database do if they wanted to make a view
efficient to query remains an open question.

\section{Ordered State}
\label{s:disc:ordered}

Certain ordered operations, like max aggregations (\texttt{SELECT MAX}) and
top-k-style queries (\texttt{ORDER BY LIMIT}), occasionally require re-fetching
underlying state as the data changes. If the maximum value in a max aggregation
or a row in a top-k view is removed, the new view content can only be determined
by re-evaluating the query.

The necessary upquery can be performed efficiently if the underlying state is
maintained in the appropriate order, but Noria does not currently support the
necessary ordered indexes. Instead, Noria provides approximate versions of such
operators. In particular, Noria's top-k operator maintains the top $2k$ items,
so that if an item is removed, the top $k$ items are still known. To get back to
$2k$ (to allow future removals), the operator fills the top view with the
highest rows it has seen so far.

This scheme avoids the need for upqueries, and works well as long as removals
from the top list are uncommon and the top list rotates over time. Otherwise,
the approach is brittle; if many top rows are removed, or if the top is changing
very infrequently, the top list may eventually hold none of the actual top
items. Support for ordered indexes, and limited upqueries against those indexes,
would fix this problem.

\section{Ranged Upqueries}
\label{s:disc:ranged}

Throughout this thesis, upqueries have been described in terms of point lookups
of the form \texttt{WHERE x = ? AND y = ?}. However, the design of partial state
is also amenable to supporting ranged queries (\texttt{WHERE x = ? AND y < ?}).
Much of the necessary work lies in changing the appropriate index structures and
including range information in upqueries, which is all straightforward. The
trickiest part of the change is to ensure that future updates are not dropped if
they fall within a requested range. For example, consider the following course
of events:

\begin{enumerate}
  \item An insert arrives with \texttt{x = 42}.
  \item An upquery arrives with \texttt{x < 50}.
  \item An insert arrives with \texttt{x = 49}.
\end{enumerate}

The second insert must be forwarded downstream so that it will update the
materialized state for \texttt{x < 50}. For Noria to realize that this is the
case, it must ``remember'' the \texttt{x < 50} upquery. More generally, it must
remember what \emph{ranges} of values are present downstream, not just what
individual keys. The solution here is to use an interval tree to track which
parts of the key space is present. An interval tree efficiently stores, merges,
and splits ranges as new ones are introduce (by new upqueries) and retired (by
evictions).

\section{Sharding Upquery Explosions}

An unfortunate phenomenon manifests itself for certain queries when partial
state and sharding combine in detrimental ways. If R is sharded differently from
Q; Q is partial; and Q's materialized ancestor, P, is sharded differently from
Q, then a miss in R may cause $k^2$ upqueries to be issued to P, where $k$ is
the sharding factor. The miss in R generates an upquery to every shard of Q, and
every shard of Q sends an upquery to every shard of P.

The three modifications above are sufficient to ensure that this situation is
handled \emph{correctly}, but additional research is needed to reduce the number
of upqueries needed. One promising avenue may be to optimize for the case where
all the shards of Q miss. If every shard of Q knows that every other shard will
upquery P, they may be able to coordinate the upqueries such that any given key
is only upqueried for once. The sharder node can then ensure that the upquery
results are sent to all the shards. This is left for future work.

\section{Emulating Partial State}

A natural question to ask is whether the benefits of partial state can be
achieved without the complexity that upqueries introduce. In particular, can a
dataflow system that supports only full materialization emulate partial state
effectively? Thoroughly exploring the answers to this question may be worth a
thesis in its own right, but some of the more obvious approaches are discussed
below.

\subsection{Lateral Joins}

The commercial materialized view stream processor Materialize~\cite{materialize}
supports \emph{lateral} joins~\cite{lateral-join}, which is described as

\begin{quote}
  [A] join modifier [that] allows relations used in a join to ``see'' the
  bindings in relations earlier in the join.
\end{quote}

In particular, lateral joins let the application author write a query that has
access to the contents of some unrelated table. For example,
Listing~\vref{l:emulate-partial-vote} shows how a lateral join can be used to
emulate a partially materialize vote count view like the one from
Listing~\vref{l:votes}. The idea here is to have a table of ``filled'' keys, and
have the results only for those keys be included in the final materialized view.

\begin{listing}[h]
  \begin{minted}{sql}
CREATE MATERIALIZED VIEW VoteCount AS
SELECT article_id, votes FROM
  (SELECT DISTINCT article_id FROM queries) filled,
  LATERAL (
      SELECT COUNT(*)
      FROM votes
      WHERE article_id = filled.article_id
  );
  \end{minted}
  \caption{Using a lateral join to emulate partial state in vote.}
  \label{l:emulate-partial-vote}
\end{listing}

This approach works well to emulate partial state in simple situations, but
requires significant manual effort for a large application. In Lobsters, for
example, the application author must re-write their applications to use such
lateral joins, and must include application logic to maintain the auxiliary
tables used to indicate what keys are materialized. It may be possible to
automate this process, though doing so requires research.

Effort notwithstanding, emulating partial state in this way also presents an
``all or nothing'' choice for applications for a given key. Either, all state
for that key is computed, or none of it is. With partial state, the state for a
key in the ultimate materialized view can be evicted without also evicting the
current vote count. The former may be significantly larger than the latter,
since it includes other columns, but is cheap to recompute. The latter on the
other hand is small, but potentially expensive to re-compute.

\subsection{State Sharing}

Partial state allows a single query of the form \texttt{WHERE x = ?} to satisfy
lookups for any value of \texttt{?}. Without partial state, the system has two
options: to remove the filter on \texttt{x} from the query and filter after the
fact, or to instantiate a separate query for each concrete value of \texttt{?}
supplied by the application. The former uses a significant amount of memory, but
is also complicated to get right; \texttt{x} may for example affect what values
are aggregated together. The latter is simpler, and uses less memory, but
requires duplicating the dataflow operators for each query, and keeping separate
state for each one.

Recent work introduced arrangements~\cite{arrangements} as a way to mitigate
this problem. Arrangements allow sharing indexes and state across related
operators to avoid duplication. However, even with arrangements, the system may
execute the same computation over a given input record more than once if it is
needed by more than one instance of a query. Furthermore, eviction is made more
difficult with arrangements, as the system must update the entire arrangement to
accommodate any new or evicted parameter value. Noria supports joint query
optimization~\cite{noria}, which combined with arrangements could reduce much of
the duplicated effort by instantiating each query multiple times, though this
does not improve the eviction process.

\section{Fault Tolerance}

If an operator's state is lost, Noria's current recovery strategy is to remove
and re-introduce the operator, and all of its descendants, as if they were new
queries. This can happen because the Noria worker hosting that operator fails,
or simply because the system is restarted. This scheme works, but means that any
past materialization work is lost and must be re-done.

A mechanism for taking snapshots of materialized state that can be recovered
later would help mitigate this. However, such a design also requires care to
ensure that any state populated \emph{since} the snapshot is correctly
incorporated. In particular, if downstream state now includes entries that
reflect data missing from the snapshot, the system must evict that downstream
state. Otherwise, updates for that data will be discarded at the recovered
operator when it discovers that the related state is missing in its state.

\section{Consistency}

Noria provides weaker consistency guarantees than many existing dataflow and
view materialization systems. This has implications for how applications use
Noria, and what behavior the application may observe.

\subsection{Write Latency == Staleness}

By design, Noria's read and write paths are disconnected from one another: reads
can usually proceed even if the write path is busy. This is both the reason why
Noria's read performance is so high, and why it gives weaker consistency
guarantees that competing systems. For example, on a 32-core machine, the
application may experience a write throughput ceiling at a few hundred thousand
updates per second, as the write path is processed by only a small number of
cores. Meanwhile, reads can happen across any number of cores; even if the write
path is entirely saturated, Noria may be able to handle millions of additional
reads per second.

While a saturated write path does not slow down the execution of queries whose
results are materialized, it does affect the read path in two important ways:
miss-to-hit time and result staleness. If a query misses, the dataflow must
compute and populate the missing state so that the read can proceed. This is the
same dataflow that handles writes, so the time until the missing read hits
instead will increase if the dataflow is busy. Similarly, while queries that do
not miss can proceed immediately, the returned results will not reflect updates
that have not yet been processed by the dataflow. Therefore, if the dataflow is
busy, the time between when an update is issued and when it is reflected in
later queries will increase.

\subsection{Transactions}

Web applications sometimes rely on database transactions, e.g., to atomically
update pre-computed values. Noria does not implement transactions, though its
support for derived views often obviates the need for them. For example, web
applications often use transactions to keep denormalized schemas synchronized: a
``like count'' column in the table that stores posts or an ``average rating''
column in the table that stores products. Noria obviates the need for such
denormalizations, and the transactions needed to maintain them, by automatically
ensuring that computed derived values are kept up to date with respect to the
base data.

\subsection{Stronger Consistency}

Noria is eventually consistent, and so is the partial state implementation
outlined in this thesis. That said, adding partial state to a system with
stronger consistency guarantees should not require extensive changes. In fact,
parts of the design could likely be simplified; union buffering, for example,
would likely no longer be necessary, and could be replaced with some kind of
multi-versioned concurrency control.

\section{Upstream Database Integration}

Existing applications that wish to adopt Noria may not want to adopt it
wholesale. They may wish to continue using their existing data backend because
they rely on its transactional properties for parts of their workload, because
they have existing backup solutions in place, or simply to make the transition
incrementally.

The most straightforward way to add Noria to an existing application backend is
to feed all changes to the primary database tables into Noria. Noria will then
maintain its copies of the base tables, with indexes it manages itself. However,
this has the downside of duplication all of the application's data between the
primary backend and Noria.

A more attractive alternative is to integrate the existing backend into Noria's
base tables. Noria would still have to be notified as changes are made to the
data so that it can propagate those changes to the maintained views, but that
data would not also have to be stored in Noria's base tables.

Unfortunately, implementing this design naively introduces a race condition:
there is now a window of time where a change that has been made to the base data
is visible to upqueries to the base tables, but the corresponding update has not
yet entered the dataflow. This is a problem, because if an upquery response
reflects that new data, and then an update arrives and adds that same data, the
data will be reflected twice!

A possible solution is to take a page out of the multi-version concurrency
control playbook, and ensure that lookups into base table state do not see the
effects of any updates that have not yet passed through its Noria operator
equivalent. Ideally, this would be based on the existing transactional
capabilities of the upstream database, but it may also be possible to emulate
using an audit table that records table changes. This is left for future work.

\section{Maintaining Downstream Systems}

Since Noria internally propagates updates that reflect deltas to past state, a
natural idea is to extend those deltas to downstream systems. For example, Noria
could notify a reactive web application when the result set for the view it is
currently showing is modified, and include in that notification what exactly
changed. In response, the application could reflect that change, all without
sending another query to the database.

Extending Noria in this way raises an interesting question around partial state.
In particular, what happens if an application ``subscribes'' to a query, and
then that query's result set is evicted? Since it is evicted, Noria will not
maintain it any longer, and the application's view will grow stale. Similarly,
what happens if the application attempts to subscribe to a query whose results
are not currently known? Or, what if the application goes offline briefly, and
now wishes to gather only the changes to the result set since it was last
online?

It may be that the solution here is simple\,---\,provide a query-and-subscribe
operator that populates missing state if needed, and then ensure that results
for outstanding subscriptions are never evicted. The view could also retain a
log of recent changes to the view to replay if a slightly stale client wants to
catch up.

\section{Eviction Strategy}

Partial state enables Noria to evict state that is infrequently accessed. It
does not dictate any particular eviction strategy as long as the partial state
invariants are maintained. In particular, if state is evicted at some operator,
any downstream state derived from the evicted state must also be evicted.

This thesis does not attempt to innovate in the space of eviction schemes, and
implements simple randomized eviction: when memory use exceeds a given
threshold, keys are evicted randomly from the three largest states in each of
the three largest domains. The number of keys is chosen proportionally to the
size of each state. This scheme works decently, and requires little coordination
or complexity, but suffers when the system runs close to capacity. Frequently
accessed keys may still be evicted due to pure chance, and when that happens the
system falls behind.

To push Noria to higher load, a smarter eviction strategy like least-recently
used should be implemented. The primary obstacle to overcome is that evictions
must happen in the dataflow write path, but the information needed to inform
eviction decisions usually come from the read path. Care must be taken to avoid
excessive synchronization between these, otherwise Noria's read performance
would be bottlenecked by the performance of the write path.

\section{Column-Based Storage}

Noria's in-memory storage is unoptimized. Specifically, every row in every state
is allocated in its own vector. This stresses the memory allocator, and
introduces non-trivial memory overhead. Since Noria knows the schema of each
view in advance, and all rows in the view have the same schema, a column-based
storage format would likely be a much better fit for many views. Noria could
even use heuristics to choose between row- and column-based storage depending on
the semantics of each operator.

\section{Cursors}

Websites frequently have paginated listings, or pages that are filled in with
more content as the user scrolls. Behind the scenes, these techniques are both
implemented using the same abstract mechanism: the cursor. There are many ways to
implement a cursor, but the most common is using the SQL \texttt{LIMIT}
operator.

On page one of a listing page with 10 results per page, the listing query is run
with \texttt{LIMIT 10}. On page two, the same query is run either with
\texttt{OFFSET 10} to skip the results from page one, or with a \texttt{WHERE}
clause that excludes the results that have already been shown. For example, if
the listing query orders results by id, the \texttt{WHERE} clause could be
\texttt{id > ?} where \texttt{?} is the last id on the previous page.

Some databases support persistent cursors as well. With a persistent cursor, the
database keeps track of what subset of the results for a query the application
has already seen, and the application can ask to read more results from that
existing cursor.

Noria currently cannot represent cursors like these since it does not maintain
the order of in-memory state (\S\ref{s:disc:ordered}). \texttt{OFFSET} might not
skip the same results as shown on the previous page, and \texttt{WHERE x > ?} is
not supported. If support for ordered state was added, Noria would support these
types of queries much like existing databases.

To make paginated queries \emph{partial}, additional challenges must be solved.
First, ranged upqueries must be implemented to support \texttt{x > ?}
conditionals (\S\ref{s:disc:ranged}). Then, a decision must be made as to how
\texttt{LIMIT} should interact with upqueries. There are two primary design
options: \emph{post-limiting} and \emph{pre-limiting}.

In a post-limited design, the query is executed without pagination-related
clauses internally, and all of its results are materialized. The limit and
offset are then applied ``at the end'': when a query execution request comes in,
only an appropriate subset of the materialized results are returned. This
solution requires no changes to the partial state logic, but also makes it
necessary to materialize all pages of each query result, even if only the first
few pages are ever accessed. Realistically, a solution that takes this approach
would therefore also include a hard upper limit on how many results are
materialized. Twitter takes an approach like this, where there is a fixed end to
each timeline that the user cannot scroll past.

In a pre-limited design, materialized state includes only results for pages that
have been accessed. This is attractive since it uses less memory, and fewer
results must be maintained. But, it also requires more complex changes to
partial state. In particular, operators must now have a way to determine if a
state change causes records to appear in, or disappear from, materialized pages
downstream. If they do not, updates may be discarded early even though they
would change downstream materialized state.

Furthermore, since intermediate operators may remove (e.g., filters) or add
(e.g., joins) rows to the resultset, the limit requested by the application may
not map directly to the number of results yielded by the corresponding upquery.
Therefore, page-specific upqueries may need to run multiple ``iterations'' to
fetch additional results if the first response did not return enough rows.

\begin{inprogress}
  Time windowing
\end{inprogress}

\section{Partial Key Subsumption}

Noria's implementation of partial state does not currently take advantage of
situations where upquery keys overlap. For example, consider the case of an
operator X where one downstream operator upqueries on column A, and another
upqueries on the pair of columns A and B. X currently keeps two indices: one on
A, and one on A+B. Each index keeps track of missing entries independently. So,
even if we previously executed and filled in an upquery for A = 3, a subsequent
request for A = 3, B = foo could miss and cause another upquery to be issued.
The operator has sufficient information that it should be able to resolve this
index miss locally, but Noria does not currently implement this optimization.
