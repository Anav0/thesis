This thesis is built on the belief that view materialization is useful, but
current implementations are too costly. It presents partial state as an
implementation that allows retaining the benefits of view materialization at a
fraction of the cost. This chapter evaluates the usefulness of view
materialization, and the efficacy of partial state:

\begin{enumerate}
  \item Why is view materialization desirable? (\S\ref{s:eval:why})
  \item Why is view materialization infeasible currently? (\S\ref{s:eval:why})
  \item Does partial state make view materialization feasible? (\S\ref{s:eval:why})
  \item Why use Noria over ad hoc caching solutions? (\S\ref{s:eval:alts})
  \item What are the trade-offs with partial state? (\S\ref{s:eval:cost})
  \item How does Noria compare to ad hoc solutions? (\S\ref{s:eval:kvperf})
  \item Does partial state speed up view creation? (\S\ref{s:eval:mig})
  \item How does skew affect the efficacy of partial state? (\S\ref{s:eval:patterns})
\end{enumerate}

\section{Experimental Setup}
\label{s:eval:setup}

The experiments in this chapter primarily use the Lobsters news
aggregator web application at \url{https://lobste.rs}~\cite{lobsters}. This
application was chosen because it is open-source (so we can see what queries it
issues), because it resembles many larger-scale applications (like Hacker News
or Reddit), and because statistics about the site's data and access patterns are
available~\cite{lobsters-data}.

The evaluation uses a workload generator that issues page requests
according to the available statistics~\cite{generator}. It does not run the
real Lobsters Ruby-on-Rails application, as the application code quickly becomes
a bottleneck. Instead, all experiments use an adapter that turns page requests
directly into the queries the real Lobsters code would issue for that same page
request. The generator supports scaling up the rate of access and user count to
emulate a larger user base for benchmarking.

\begin{table}
  \begin{tabular}{ p{0.8in} | r | r | r | p{2.9in} }
    Page & \% & W & Q & Description \\
    \hline
    Story & 55.8 & 1 & 14 & Renders an individual story's page, including its
    popularity score, comments, and the scores of its comments.\\
    Front page & 30.1 & 0 & 14 & Lists the 25 most highly scored stories, along
    with their authors and scores.\\
    User & 6.7 & 0 & 7 & Renders a user summary page, including what story
    ``tags'' they contribute to.\\
    Comments & 4.7 & 0 & 9 & Like the front page, but for comments.\\
    Recent & 1.0 & 0 & 14 & 25 most recently added stories, along with their
    authors and scores.\\
    Vote & 1.2 & 1 & 2 & Vote up/down a given comment or story.\\
    Comment & 0.4 & 2 & 5 & Add a new comment to a story.\\
  \end{tabular}
  \caption{Pages in Lobsters. \% indicates the percentage of requests that load
  the given page. W is the number of writes performed by a given page. Q is the
  number of (read) queries a page issues.}
  \label{t:lobsters-pages}
\end{table}

The various pages in Lobsters differ in what queries they issue, how many
queries they issue, and the extent to which they are read or write heavy.
Table~\vref{t:lobsters-pages} gives an overview of how often each page is
accessed and what loading each page entails. In all evaluation results, latency
is measured across all requests, no matter what page they are for.

Experiments run on Amazon EC2 r5n.4xlarge instances, which have 16 vCPUs and
128GB of memory. The server is always given a dedicated host, while
load-generating clients are split across one or more m5n.4xlarge instances
depending on the desired load factor.

The benchmarks are all ``partially open-loop''~\cite{frank-open-loop}:
clients generate load according to a workload-dictated distribution of
interarrival-times, and has a limited number of backend requests outstanding,
queuing additional requests. This ensures that clients maintain the measurement
frequency even during periods of high latency. The test harness measures offered
request throughput and ``sojourn time''~\cite{open-loop-cautionary-tale}, which
is the delay the client experiences from request generation until a response
returns from the backend.

To capture the variance of measurements, the benchmarks use
HdrHistogram~\cite{hdrhistogram}, a data structure that efficiently captures and
represents histograms with a high dynamic range over large numbers of samples.

All experiments report the resident virtual memory of the server process (VmRSS)
unless otherwise noted. This measurement therefore includes all indexes, runtime
allocations, and other bookkeeping metadata. For Noria, it also includes the
data stored in the base tables except where indicated.

Since the benchmarks introduce more data as they run, memory use increases with
time. Experiments are run for 5 minutes unless otherwise specified, and memory
measurements are taken at the end of the run. All results are stable and
consistent across multiple runs.

\section{Benefits of Partial View Materialization}
\label{s:eval:why}

The core argument of this thesis is that partial state makes materialized views
usable as caches. That argument intertwines several questions that must be
answered before further evaluation of partial state is interesting:

\begin{enumerate}
    \item Why is view materialization desirable?
    \item Why is view materialization infeasible currently?
    \item Does partial state improve on this situation?
\end{enumerate}

\begin{figure}[h]
  \centering
  \includegraphics{graphs/lobsters-throughput.pdf}
  \caption{Maximum achieved throughput on Lobsters benchmark with and without
  view materialization. Without view materialization, MySQL must compute query
  results each time. Traditional (full) view materialization runs out of memory
  at $\approx$4.6k pages/second. Partial state allows Noria to reduce memory use
  significantly so that it can achieve higher throughput.}
  \label{f:lobsters-throughput}
\end{figure}

Figure~\vref{f:lobsters-throughput} attempts to explain why view materialization
is desirable. It compares the highest sustainable request load of three
different systems: MySQL, Noria without partial state, and Noria with partial
state. MySQL is run entirely in RAM by running it on a ramdisk, and on its
lowest isolation level. The figure shows the highest Lobsters throughput each
system achieves before its mean latency exceeds 50ms.

View materialization alone (as provided by Noria) improves performance by almost
$11\times$ compared to MySQL, as query results are now frequently cached.
However, without partial state, this performance increase comes at a significant
memory cost. Beyond 4.6k pages/second, Noria without partial state runs out of
memory, and cannot support the workload. With partial state, Noria uses much
less memory at a given load factor, which allows it to support 67\% higher
throughput, over 18$\times$ that of MySQL%
\footnote{The Noria benchmarks are memory-constrained, not CPU-constrained.
MySQL fully loads all 16 cores at 417 pages per second.}.

\begin{figure}[h]
  \centering
  \includegraphics{graphs/lobsters-memory.pdf}
  \caption{Memory use two minutes into the Lobsters benchmark at 4.6k pages per
  second. Right bar in each pair show memory use when base tables are stored on
  disk using RocksDB.}
  \label{f:lobsters-memory}
\end{figure}

Figure~\vref{f:lobsters-memory} shows the memory use at 4.6k pages per second
with and without partial state. It demonstrates both the issues with full
materialization, and the improvements brought about by partial state. With full
materialization, Noria must store every result for every query in memory. In
contrast, with partial state, Noria stores only frequently accessed results,
which cuts memory use in half.

The memory use reductions with partial state are a direct result of the skew in
Lobsters data popularity and access patterns. Many pages are simply never
visited over the course of the benchmark, and so need not be brought into the
cache. With partial state, Noria also evicts infrequently accessed results,
which further reduces memory use, and ensures that the cache does not eventually
grow to contain all results.

Much of Noria's memory use goes to storing the base tables in memory. Since
partial state cannot evict base table state, this limits how much memory can
potentially be saved. Figure~\ref{f:lobsters-memory} therefore also includes
memory use when running Noria with its durable RocksDB storage backend for base
tables. In that configuration, base tables are kept on disk, not in memory,
which makes the memory savings from partial state more apparent\,---\,the memory
use is now about a third that without partial state.

Various other runtime overheads that partial state cannot eliminate remain, such
as data structures and allocations for in-flight requests and pending responses.
With diligent memory optimization, this overhead could likely be further reduced
to increase the relative benefits from partial state.

\begin{figure}[h]
  \centering
  \includegraphics{graphs/lobsters-opmem.pdf}
  \caption{Estimated operator state data size two minutes into the Lobsters
  benchmark at 4.6k pages per second. The value indicated includes only the sum
  total size of rows in each operator's state, not data structure overheads,
  indices over the data, or other memory allocations. Base tables are not
  included.}
  \label{f:lobsters-opmem}
\end{figure}

To provide some insight into how far memory use can be reduced,
Figure~\vref{f:lobsters-opmem} shows the total size of the data contained in
non-base operator state in Lobsters. This metric measures \emph{only} the sum of
the data in each row, and excludes other memory overheads such as hash tables,
additional indices, or allocations elsewhere in the application. The results
indicate that partial state in isolation requires only 1.5\% of the total
operator state to be materialized; significantly less than the $\sfrac{1}{3}$
seen in Figure~\ref{f:lobsters-memory}. This suggests that there is indeed
potential for reducing partial state's memory footprint.

Since partial state uses less memory, applications that do not need higher
throughput can instead reduce cost by using hosts with less memory. For example,
on AWS EC2, a 16-core instance with 128GB of memory is 25\% more expensive than
the same with 64GB of memory. A host with 256GB of memory is twice the price of
a 128GB host.

\paragraph{In summary,} Noria's view materialization increases Lobsters
throughput by an order of magnitude. Partial state cuts memory by more than
50\%, doubling achievable throughput on the same hardware, or enabling
cost-cutting by using server machines with less memory.

\section{Rolling Your Own}
\label{s:eval:alts}

Many applications already require lower latency and higher throughput than
straightforward SQL queries against traditional relational databases provide.
In an attempt to bridge the gap, developers often implement manual optimizations
to improve their application performance, and introduce additional complexity
into their applications in the process. These optimizations usually come in one
of two forms: denormalization and caching. This section discusses each of these
optimization techniques in turn, as well as how Noria makes them unnecessary.

\subsection{Denormalization}

The relational database model~\cite{relational} encourages developers to use
a \emph{normalized} schema in which redundant data that can be derived from
other data is not stored. Instead, the model suggests that derived data be
computed on demand using standard relational operators. The paper goes on to
add:

\begin{quote}
Only in an environment with a heavy load of queries relative to other kinds
of interaction with the data bank would strong redundancy be justified in the
stored set of relations.
\end{quote}

As discussed in the motivation section for this thesis, many web applications
fall into exactly this category. Queries are far more common than inserts or
updates, and with a normalized schema they must constantly expend resources to
re-compute such derived data. For this reason, web developers often explicitly
denormalize their schema to include data that would be prohibitively expensive
to compute on-demand.

For example, in Lobsters, each story has a ``hotness'': a score of how popular a
story is, and thus how far up it should appear in listings. This value depends
on a lot of parameters, such as the number of votes, the number of comments, the
score of those comments, etc. It would be prohibitively expensive to compute a
story's hotness directly in the queries, especially in the context of computing
the front page view, which requires the hotness for \emph{all} stories to rank
them. Instead, the Lobsters developers chose to add a computed column,
\texttt{hotness}, to the \texttt{story} table. This column is then updated
whenever relevant data changes, such as when:

\begin{itemize}
    \item a story is upvoted or downvoted;
    \item a comment is added to a story;
    \item a comment on a story is upvoted or downvoted;
    \item a comment or vote is deleted; or
    \item one story is merged into another.
\end{itemize}

There are several such computed columns in Lobsters. For each one, developers
had to inspect each write path and modify them to ensure that they correctly
update all related computed values. This process is manual and error-prone, but
also necessary: without them, the Lobsters experiment run against MySQL
\textbf{cannot keep up with even 1 request per second}.

With Noria, such manual denormalization is unnecessary. View materialization
automatically stores and maintains derived data so that it is efficient to
query. The developer can continue to use normalized schemas and queries, and
does not need to modify their application code to manage denormalized columns
and tables.

\subsection{Caching}

If denormalization does not sufficiently improve the application's performance,
the next step is usually to add a cache in front of the database. This cache
often takes the form of a key-value store, like Redis or Memcache, which holds
frequently accessed, computed results. When the application issues a query, it
checks the (fast) cache first, and only if the results are not available in
cache is the (slower) backend consulted.

A dedicated cache speeds up repeated reads, but introduces significant
application complexity. Just like for manual denormalization, all parts of the
application that modify data related to any given cache entry must know to also
invalidate or update the cache. In addition, the developers must ensure
that if multiple clients miss on a given entry, they do not hit the backend
database all at once. This is especially important if a popular entry is
invalidated, as it may cause a ``thundering herd'' effect where a large number
of clients swarm the backend and overwhelm it. Furthermore, since the clients
must now access two separate systems, mechanisms must be in place to ensure that
the cache remains consistent with the underlying data. This is difficult since
data may be updated at any time, including \emph{just} after a client has
fetched the (then) latest data from the database.

Because of the challenges above, implementing caching ``correctly'' requires
highly sophisticated machinery~\cite{facebook-memcache, txcache, orm-cache,
sql-cache}, which developers may not even think to employ. A survey from 2016
found that 0.3-3.0\% of application code spread across 2.1-10.8\% of the
application's source files is caching-related, and that cache-related issues
make up 1-5\% of all issues~\cite{caching-is-hard}.

With Noria, there is no need to maintain such a query result cache; Noria's
in-memory materialized views provide high-throughput, low-latency queries
directly from the database. Thanks to partial state, Noria's materialized views
are usable even for applications whose full cache state exceeds the amount of
memory available on the server host. Since Noria automatically maintains the
materialized views, the application also does not need code to manage cache
invalidation, or to address challenges like thundering herds.

\paragraph{In summary,} without Noria, manual performance optimizations like
denormalization and query result caching are necessary, but error-prone and
labor-intensive. Noria obviates the need for both.

\section{Partial State's Memory Trade-off}
\label{s:eval:cost}

Partial state's main drawback compared to complete materialized views is
that the results for an application's query may not be known. Or, stated
differently, some reads may miss. When this happens, the system must upquery the
missing state, which takes time and consumes resources otherwise dedicated to
writes. This shows up as increased tail latency for the application: queries
whose results are not known must wait to be computed. The hope with partial
state is that, once the commonly-accessed query results are cached, latency
quickly drops such that only infrequently accessed query results must be
computed on-demand in the future.

\subsection{Warming the Cache}

The cost of these misses is particularly visible when Noria starts with empty
state. This is equivalent to starting a more traditional caching system with an
empty (cold) cache, and having to ``warm'' it by filling in the most popular
entries. To measure this warming period, Figure~\vref{f:lobsters-timeline} shows
the latency profile seen by the Lobsters benchmark over time, starting at the
point when the first query is issued. Time increases along the x-axis, and the
measured latency for each time bin is plotted on a logarithmic scale on the
y-axis. Lighter colors include more of the tail.

\begin{figure}[t]
  \centering
  \includegraphics{graphs/lobsters-timeline.pdf}
  \caption{Lobsters latency profile at 1.5k pages per second over time, starting
  when the first query is issued. Time increases along the x-axis, and each bin
  samples twice as long as the last. Later bins therefore capture more variance.
  The bin latency is plotted on a logarithmic scale. Lighter colors include more
  of the tail.}
  \label{f:lobsters-timeline}
\end{figure}

The figure shows that latency is initially high, but after a few seconds, the
mean and 95th percentile latency drop below 10 milliseconds. By the time a
minute has passed, the 99th percentile has followed suit. Since only a small
portion of the total computed state is cached (as shown in
Figure~\vref{f:lobsters-memory}), this experiment supports the hypothesis that
partial state achieves low latency once the most commonly accessed results are
cached. The remaining latency is primarily determined by the number of queries
each page issues, as each one requires a round-trip to Noria.

\subsection{Paying with Tail Latency}

\begin{figure}[h]
  \centering
  \includegraphics{graphs/lobsters-memlimit-cdf.pdf}
  \caption{CDF of sojourn latency in Lobsters at 1.5k pages per second as
  a function of eviction aggressiveness. The figure depicts steady-state
  operation\,---\,the benchmark has been allowed to run for two minutes before
  the latency is measured. Top figure uses a logarithmic x-scale to highlight
  the full range of the tail latency.}
  \label{f:lobsters-mem-latency}
\end{figure}

Partial's trade-off is that of memory use versus tail latency; with less memory,
Noria precomputes less of the tail, and thus more requests must be computed
on-demand. Figure~\vref{f:lobsters-mem-latency} shows this trade-off in the
steady state of the application by plotting the CDF of the sojourn latency
across all requests with increasingly aggressive eviction.

As Noria reduces memory use by evicting more aggressively (darker lines), more
requests take a long time, and tail latency increases. In other words, the
lower the memory use, the higher the tail latency. The ability to trade off tail
latency for reduced memory use is the primary benefit of partial state; without
it, requests in the tail are always fast, but all the materialized views must
fit in memory.

The reason why the whole curve shifts, rather than just the tail, is that these
CDFs are across \emph{all} the different page types in Lobsters. Each one issues
a different set of queries, and so their total time differs, as does the effect
of a longer tail. The exact shape of this curve, and how it shifts in response
to varying resources, depends on the application in question.

\subsection{Upqueries to Disk}

\begin{figure}[h]
  \centering
  \includegraphics{graphs/lobsters-durability-cdf.pdf}
  \caption{CDF of sojourn latency across all Lobsters pages at 1.5k pages per
  second with base tables in memory and on disk. The figure depicts steady-state
  operation\,---\,the benchmark has been allowed to run for two minutes before
  the latency is measured.}
  \label{f:lobsters-dur-latency}
\end{figure}

If base tables are not kept in memory, the cost of recomputing missing state
from the data in those base tables increases. Exactly how much depends on the
performance characteristics of the durability backend in use.
Figure~\vref{f:lobsters-dur-latency} shows a CDF of page latencies when Noria's
RocksDB backend is used, backed by a ramdisk. Latencies increase by around 20\%,
and varies depending on the number of misses a given page request experiences.

\subsection{Memory Use and Throughput}

Memory use can only be reduced so far before the system no longer keeps up with
the offered load. If some of the most frequently accessed query results are not
cached, the system will constantly have to re-compute those results to satisfy
reads that come in shortly after that query result is evicted. This cache churn
increases latency and decreases throughput, often significantly. Essentially,
the system will never finish warming the cache, and latency will remain at the
high levels shown early in Figure~\ref{f:lobsters-timeline}. For Lobsters, this
happens around the 18GB mark. If the eviction is tuned to be more aggressive
than that, Noria can no longer sustain 1.5k pages per second.

Generally speaking, as throughput increases, so must the memory budget, since
the memory budget effectively dictates the hit rate. The more requests issued
per second, the more misses (in absolute terms) result from a given hit rate. If
those misses in the tail are distinct, Noria must satisfy \textbf{more}
upqueries as load increases, while also handling that added load.

\begin{listing}[h]
  \begin{minted}{sql}
SELECT stories.*, COUNT(votes.user) AS nvotes
FROM stories
LEFT JOIN votes ON (stories.id = votes.story_id)
GROUP BY stories.id
WHERE stories.id = ?
  \end{minted}
  \caption{Simplified query for vote counting in Lobsters. Effectively the same
  as Listing~\vref{l:vote-src}.}
  \label{l:votes}
\end{listing}

While this correlation between throughput and memory use exists in Lobsters, it
is difficult to show clearly as each page issues many different queries, and
overall load is relatively low. For this reason, the next set of benchmarks use
a simplified version of one particular query from Lobsters shown in
Listing~\vref{l:votes}. The rest of the thesis refers to this as the ``vote
benchmark''. It counts the number of votes for a story, and presents that
alongside the story information. The benchmark issues requests distributed as
99\% reads and 1\% writes (inserts into \texttt{votes}). The access pattern is
skewed such that 90\% of requests access 1\% of keys across 10M stories%
\footnote{Specifically, it samples keys from a Zipfian distribution with a skew
factor ($\alpha$) of 1.15.}. Load is generated by four clients, and each one
batches requests for a maximum of 10ms to reduce serialization overheads.

\begin{figure}[h]
  \centering
  \includegraphics{graphs/vote-throughput-memlimit.pdf}
  \caption{Achieved throughput vs 95th percentile request latency in vote with
  increasingly aggressive eviction. Offered load increases along the points on
  each line. A near-vertical line indicates that the system no longer keeps up
  with offered load.}
  \label{f:vote-throughput-memlimit}
\end{figure}

Figure~\vref{f:vote-throughput-memlimit} demonstrates the connection between
throughput and memory use. It shows throughput-latency lines for the vote
benchmark with progressively more aggressive eviction. Each point along each
line is a higher offered load; the point's x-coordinate is the achieved
throughput, and its y-coordinate is the measured 95th percentile latency. When
Noria no longer keeps up, you see a ``hockey stick'' effect, where achieved
throughput no longer increases, while latency spikes. The figure shows that as
the offered load increases, Noria needs to use more memory to keep up.

\paragraph{In summary,} partial state enables applications to improve their
tail latency and throughput by ``paying'' with memory. Noria takes advantage of
additional memory to further reduce tail latency and increase sustainable
read throughput. Assuming applications see sufficient skew, like in Lobsters,
the cache warms up quickly.

\section{Cache Lookup Performance}
\label{s:eval:kvperf}

Despite how error-prone the approach is (\S\ref{s:eval:alts}), ad hoc
application caching is still common in practice. To present a viable
alternative, Noria must not only reduce the developer burden of getting caching
right; it must also offer competitive performance with manually constructed
caching solutions.

Unfortunately, this is difficult to evaluate, since high-performance solutions
are often custom-built for a given application, and not available as
general-purpose tools. Effectively applying the general-purpose tools that
\textbf{are} available, like Memcache and Redis, requires significant effort on
the part of the application authors (or the evaluators). To manually add caching
support to Lobsters' ~80 queries, including thundering herd mitigation and
incremental updates, would be a significant undertaking.

This fact alone is, in essence, an argument for the Noria approach. The manual
effort involved in making Lobsters use Noria is minimal\,---\,just switch the
code to query Noria instead of MySQL, and get automatic caching. In many cases
the application code can even be simplified, such as by removing denormalized
schema modifications (and the associated maintenance code) like the story
``hotness'' column described in \S\ref{s:eval:alts}.

Nevertheless, an experiment to evaluate Noria's absolute performance compared to
a ``real'' cache is necessary. Without such a comparison, Noria can only claim
to be ``faster than MySQL'', but not ``as fast as a cache''.

The next experiment runs the vote benchmark from Listing~\vref{l:votes} against
Redis~\cite{redis}, a popular high-performance key-value store that is commonly
used as a caching backend. In an attempt to approximate how a carefully planned
and optimized application caching deployment might perform, it makes the
following modifications to the benchmark:

\begin{itemize}
 \item Every access hits in cache, to emulate perfect thundering herd mitigation
   and invalidation-avoidance schemes.
 \item Nearly all accesses (99.99\%) are reads, since writes would be
   bottlenecked by the backing store.
 \item Data is not stored anywhere except in Redis.
 \item Accesses are batched to reduce serialization cost and increase
   throughput. Specifically, reads are \texttt{MGET}s, and writes are pipelined
    \texttt{INCRBY}s.
\end{itemize}

This is not a realistic use of Redis as a cache, and ignores the complexities of
integrating the cache with the application. It also assumes that cached query
results are never spread across more than one key in the cache. However, it
enables an evaluation that assumes the best about the implemented caching
strategy and system.

\begin{figure}[h]
  \centering
  \includegraphics{graphs/vote-redis.pdf}
  \caption{Achieved throughput vs 95th \%-ile request latency in cache-optimized
  vote. Offered load increases along the points on each line. The vertical
  line indicates $16\times$ the highest Redis throughput, since Redis is
  single-threaded.}
  \label{f:vote-redis}
\end{figure}

Figure~\vref{f:vote-redis} shows a throughput-latency plot that explores the
performance profiles of Redis and Noria under these experimental conditions%
\footnote{Noria runs with the same modified access patterns as outlined for
Redis.}.
% For comparison, it also includes a MySQL + Redis implementation that
% stores votes and articles in MySQL, and uses the na\"ive write-back cache
% strategy outlined above.
Redis is not multi-threaded, and can only use one of the server's 16 cores, so
the figure also includes the Redis performance extrapolated to 16 cores. This is
an over-estimate, since to achieve this performance in practice, the
application's already-perfect caching scheme would need to also shard
perfectly\footnote{This is also the reason why Redis was chosen over
memcached\,---\,Redis' single-core implementation avoids all concurrency
overhead, and so $16\times$ its performance is likely to provide a better
estimate of a reasonable maximum.}. Noria implements the necessary
synchronization internally to take advantage of all the cores without sharding.

The results show that Noria achieves about $\sfrac{2}{3}$ of the theoretical
16-core performance of Redis. Given the idealized nature of this experiment, the
exact absolute numbers should be taken with several grains of salt, but they do
provide an upper bound of sorts for Redis' performance. That Noria approaches
this performance is a good indicator that Noria's cache hit performance is
comparable to that of an ad hoc caching implementation. And again, Noria does so
while providing rich SQL queries, and without requiring application-specific
caching logic.

\paragraph{In summary,} Noria's absolute lookup performance is comparable to
that achievable by using Redis as an ad hoc query cache.

\section{Bringing Up New Views}
\label{s:eval:mig}

When the application issues a query that Noria has never seen before, Noria must
instantiate the dataflow for that query, along with any materializations it
might need. Without partial state, the system must also do all the work to
compute the full state for the new view, and any internal operator state it
depends on, up front and all at once. And during that time, Noria's dataflow
must spend cycles on computing that new state, slowing down the processing of
other concurrent writes. The new view also cannot serve any reads until all the
state is computed.

\begin{listing}[h]
  \begin{minted}{sql}
CREATE VIEW scores AS
  SELECT votes.story_id, COUNT(votes.user) AS score
  FROM votes
  GROUP BY votes.story_id
UNION
  SELECT ratings.story_id, SUM(ratings.rating) AS score
  FROM ratings
  GROUP BY ratings.story_id;

SELECT stories.*, SUM(scores.score)
FROM stories
 LEFT JOIN scores ON (stories.id = scores.story_id)
GROUP BY stories.id
WHERE stories.id = ?;
  \end{minted}
  \caption{Updated query for ``rating'' counting in Lobsters.}
  \label{l:ratings}
\end{listing}

Partial state enables such query changes to be instantaneous in many
cases\,---\,if the new view can be made partial, Noria makes it empty and
immediately available. Noria then fills it on demand as the application submits
reads. To demonstrate the difference in behavior between with and without
partial state for migrations, the next benchmark modifies the ``vote'' benchmark
from Listing~\ref{l:votes}. It introduces a new table, \texttt{ratings}, which
has ratings on a scale from 0 to 1 for each story instead of just a vote of 0 or
1. It also add a new view, shown in Listing~\vref{l:ratings}, which combines the
existing votes with the new ratings to compute a total story score\footnote{By
writing the query this way, votes and ratings can co-exist.}.

The benchmark inserts votes and issues the original vote query for 90 seconds,
and then introduces the new table and query from Listing~\ref{l:ratings}
(denoted as time 0). From then on, it issues both votes and ratings, and queries
both views every 10 milliseconds.

\begin{figure}[h]
  \centering
  \includegraphics{graphs/vote-migration.pdf}
  \caption{Top: Setting up and access a new view.\\ Bottom: Write performance
  across the migration.\\ Access pattern is skewed such that 90\% of accesses
  are for 10\% of 10M stories (Zipf; $\alpha$=1.15). Benchmark runs for 90s
  prior to migration (solid vertical line). The dashed vertical line denotes the
  end of the migration without partial state.}
  \label{f:vote-migration}
\end{figure}

Figure~\vref{f:vote-migration} plots the cache hit rate seen by reads from the
new view over time (top), as well as the write throughput over the course of the
experiment (bottom). Without partial state, the new view is not accessible until
its construction finishes after $\approx$23 seconds. During that time, the
application write performance drops substantially, as Noria must compute the
content of the new view.

With partial state, the view is immediately accessible, though its cache hit
rate is initially low. However, since there are a few very popular keys, the hit
rate quickly climbs to over 90\%. As only results for requested keys are
computed, write throughput is mostly unaffected by the migration%
\footnote{Overall write throughput is lower after the migration since Noria must
now maintain two views, not just one.}.

The figure also exposes another interesting effect of using partial
materialization: increased write throughput. Without partial state, every write
must be processed to completion, since all results are cached. With partial
state, writes for keys that have not been read can be discarded early, as there
is no state in memory that must be updated, which increases throughput.

\paragraph{In summary,} partial state enables fast adoption of new views without
compromising the performance of concurrent writes. Such partial views also
quickly satisfy most requests. In addition, by maintaining only a subset of
computed state, partial state increases write throughput, since entries that are
not in the cache do not need to be updated.

\section{Skew}
\label{s:eval:patterns}

Partial state is mainly useful if accesses are skewed towards a particular
subset of queries and data. When this is the case, caching a small subset of the
application's computed state speeds up a significant fraction of requests. If
this is not the case, the likelihood of missing in the cache is inversely
proportional to the size of the cache, and you would need to cache computations
over most of the data to maintain a decent cache hit rate.

Lobsters is skewed, which is what allows Noria to run it smoothly even when only
a small fraction of results are cached. Significant skew shows up across a wide
range of other real-world datasets~\cite{power1, power2, network-skew,
large-skew-analysis}, including many social networks~\cite{network-skew2,
community-skew}. In a large public Amazon data set~\cite{amazon-skew}, the
100,000 most popular book titles (less than 5\%) account for roughly 50\% of all
book sales, and 75\% of the sales are for the top 500,000
titles~\cite{zhang2020permutation}.

In the vote benchmark, which story to fetch and vote for is artificially
skewed using a Zipfian probability distribution~\cite{zipf}; a probability model
that describes skewed frequency distributions in many natural and random
datasets. Given some number of elements $N$ and a skew parameter $\alpha$, the
normalized frequency of the $k$th element in a Zipf distribution is given by:

\begin{displaymath}
  f\left(k;\alpha,N\right)={\frac {1/k^{\alpha}}{\sum \limits _{n=1}^{N}(1/n^{\alpha})}}.
\end{displaymath}

Every time the vote benchmark performs a read or a write, it samples a value,
$k$, in such a way that the likelihood of choosing a given $k$ is given by
$f\left(k;\alpha,N\right)$. A higher value for $\alpha$ means that smaller $k$
values will be sampled more frequently than larger $k$ values, increasing the
skew.

It is difficult to estimate the degree of skew for a complex application ahead
of time. But, because many datasets exhibit skew following something akin to a
Zipfian distribution, an analysis of the vote benchmark may still yield some
helpful heuristics for application developers.

After $S$ samples (throughput $\times$ time), the expected number of keys hit is
the sum of the probability that each $k$ is sampled at least once, given by:

\begin{displaymath}
  F(\alpha,N)={\sum \limits _{k=1}^{N} \left(1 - \left(1 - f(k; \alpha, N)\right)^{S}\right)}.
\end{displaymath}

\begin{figure}[h]
  \centering
  \includegraphics{graphs/vote-formula.pdf}
  \caption{Probabilistic model of the fraction of 10M keys that are accessed
  over the course of one second as throughput increases. Each line shows a
  different amount of skew. Skew (X/Y) denotes that X\% of requests come from
  Y\% of keys. More keys pushes the curve down.}
  \label{f:vote-formula}
\end{figure}

Figure~\vref{f:vote-formula} plots $F(\alpha, N)/N$ after one second for
different degrees of skew ($\alpha$) with $N=10\text{M}$ as throughput varies.
One second was chosen as this is how often Noria's eviction code runs. That
value corresponds to the expected fraction of keys accessed between any two
eviction cycles, and effectively sets a lower bound on the fraction of the query
results that must be cached. It thus also dictates minimum memory use. While
Noria \emph{could} maintain a smaller fraction of the query results, the
application would likely need those keys again shortly after evicting them. This
would cause significant churn, where Noria would continuously compute and then
discard frequently accessed query results.

Also indicated on the figure is the cache fraction in the vote benchmark at 250k
operations per second, as measured by the metric used in
Figure~\ref{f:lobsters-opmem}: the operator state data size. The benchmark runs
smoothly with 90k of 10M stories cached, which is close to the 46k stories
computed by the formula for the 90/1 skew that the benchmark uses. If the
eviction is tuned to be even more aggressive, the benchmark no longer keeps up.
This suggests that Noria is indeed able to function at close to the predicted
cache ratio, and that the model may be useful in estimating achievable memory
savings.

At very high offered load, Noria can rarely get quite as low as the graph
indicates. For example, at 1M operations per second, Noria must maintain 28\% of
keys to keep up, even though the model predicts that 1.4\% should be sufficient.
There are multiple related reasons for this.

First, Noria currently implements \emph{randomized} eviction, so frequently
accessed keys will occasionally be evicted. When they do, many requests must
wait for its result to be recomputed. With a less naive eviction scheme, such as
LRU, such evictions can be avoided, and hot keys will never miss.

Second, more upqueries must be serviced per second. Since upqueries are
performed by the dataflow, which is single-threaded along any given path, the
upquery processing itself becomes a bottleneck. To maintain acceptable latency,
Noria is forced to keep many more keys in cache than the model predicts so that
not too many upqueries occur.

And third, Noria's eviction runs at a fixed interval of one second. As offered
load increases, so too does the number of keys read, and the number of keys
cached in that one second. The eviction logic thus has more keys it needs to
evict each time it runs. This in turn takes up more data flow cycles on the
write path that could otherwise be dedicated to serving upqueries.

\paragraph{In summary,} Noria benefits from skewed access, which is common in
real-world datasets. At moderate throughput levels, Noria falls over only when
asked to evict more keys than the predicted size of the ``hot'' key set.

\section{Cost of (Partial) View Maintenance}
\label{s:eval:writes}

View maintenance is not free: writes to traditional relational databases need
only modify the contents of a single table, while in Noria those changes must
propagate through the dataflow. What may have started as a single new table row
may cause a host of updates to different views, dependent upqueries, and
incongruent join evictions. Thus, while Noria \emph{reads} are faster than reads
from traditional databases, \emph{writes} are slower.

For applications whose workload skews towards reads, this trade-off still tends
to be worthwhile; more CPU cycles are saved from not repeatedly re-executing
queries for reads than are consumed by processing writes through the dataflow.
However, as the application grows and its load increases, \emph{eventually} its
write volume may still become a bottleneck. This is because while Noria can
process reads in parallel on many cores, writes must flow through the dataflow,
which has less capacity for concurrent processing. Ultimately, \emph{some} node
in the dataflow will be unable to keep up with the write load offered to it, and
a queue will build up upstream of that node.

Where this bottleneck occurs depends on the application workload, as well as how
the application writes arrive. If writes are also skewed, and arrive in batches,
Noria's operators can sustain higher throughput than if the writes are uniformly
distributed or arrive in small batches at high frequency.

The reason why Noria plateaus where it does in
Figure~\vref{f:lobsters-throughput} is because of precisely such a bottleneck in
the Lobsters queries. In particular, Lobsters has a query that fetches a user's
\emph{notifications}. A notification is generated for user $u$ whenever another
user posts a direct response to user $u$'s story or comment \textbf{after $u$
last viewed that story}. This last part is key, because it requires that every
time a user visits \emph{any} story, that visit must update the user's
notifications in case any should be removed. Thus, 55.8\% (cf.
Table~\vref{t:lobsters-pages}) of requests must send an update through the one
dataflow path that updates the notifications view.

When Lobsters latency spikes beyond 7700 pages per second, it is this dataflow
update that drives up the latency. The server has spare memory and spare cores,
but one thread is constantly busy working on the notification
dataflow\footnote{This load could be spread across cores by sharding the
dataflow, though this would slow down other queries due to limitations in
Noria's basic sharding implementation.}. The $\approx4300$ story requests per
second generate over 100,000 reads per second, which Noria handles just fine,
but the 4300 \emph{updates} per second saturate Noria's dataflow for the
notifications query. This suggests that Noria's dataflow cannot support an
aggregate update rate beyond 4300 across tables that share a downstream dataflow
node.

Partial state has a modest impact on what update rate Noria can support. Without
partial state, \emph{every} update must be processed to completion, which
reduces throughput as shown in Figure~\vref{f:vote-migration}. At the same time,
\emph{with} partial state, the dataflow must also service upqueries for missing
state in downstream materializations, which reduces the effective update rate
that dataflow can sustain.

\paragraph{In summary,} Noria reduces the cost of reads, but increases the cost
of writes in order to do so. Beyond $\approx4300$ updates per second to a single
segment of the dataflow, the write processing pipeline becomes a bottleneck, and
must be sharded or otherwise modified to support higher update rates.
