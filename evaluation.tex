The work in this thesis is built on the belief that materialization is useful,
and that partial materialization is necessary to make it practical for an
important class of applications. Without materialization, queries must either be
manually cached, which is labor-intensive and error-prone, or be computed on
demand, which is slow. And with full state materialization, all queries are
fast, but the memory cost is prohibitive for any but the smallest applications.
Whereas partial state allows you to trade off application tail latency for
reduced memory use.

In this section, I provide experimental and qualitative evidence that this
belief is warranted through analysis of the following questions:

\begin{enumerate}
 \item What impact does partial state have on memory use? \S\ref{s:eval:mem}
 \item What is the cost of using partial state? \S\ref{s:eval:cost}
 \item How dependent is partial state on application access patterns? \S\ref{s:eval:patterns}
 \item How does partial state compare to existing solutions? \S\ref{s:eval:existing}
\end{enumerate}

The high-level take-away from this section is that partial state allows
read-heavy applications with skewed access patterns to operate using
significantly less memory, with marginal impact on system throughput and
latency.

\section{What impact does partial state have on memory use?}
\label{s:eval:mem}

The primary motivation behind reducing application memory use is to reduce cost.
Higher memory use requires more memory, and more memory costs more, both to
purchase and in ongoing power consumption. An application that uses less
memory can scale further on the same hardware, or, conversely, can run on
smaller hardware than would otherwise be needed.

The key question for partially stateful dataflow then is whether it truly and
meaningfully reduces application memory use. Or, more accurately, whether
applications can run efficiently when only some of its query results are cached.
Intuitively, one might expect this to be true for many user-driven applications:
some queries may only be executed for rarely used features, and unpopular posts
may be accessed only very infrequently, or perhaps not at all, after they were
initially posted.

To validate this, I used production statistics from the open-source Lobsters
news aggregator~\cite{lobsters,lobsters-data} to build a benchmarking harness
that approximates the page access patterns the real Lobsters website sees. The
load generator is ``partially open-loop'' to ensure that clients maintain the
measurement frequency even during periods of high latency, and also measure
queueing time~\cite{frank-open-loop,open-loop-cautionary-tale}. Since Lobsters
is a small site, relatively speaking, the harness also lets me to proportionally
scale up the access frequency to evaluate a larger deployment with the same
data and query access patterns.

I then wrote a Noria implementation for each Lobsters page endpoint that
queries the same data, and performs the same writes as the real Ruby-on-Rails
application.%
\footnote{I did not run the Ruby-on-Rails code directly to avoid measuring
bottlenecks in Ruby and Rails.}
Together, these allow me to measure the throughput, latency, and memory overhead
that Noria would provide if used as the backend for Lobsters as it scales up.

Figure~\ref{f:lobsters-memory} shows the memory use%
\footnote{Note that this counts the data set size, not the resident virtual
memory.}
of stateful operators with
and without partial materialization at a scale-up factor of 4000 times normal
Lobsters load. Note that the figure does not show the memory used by the base
tables, as it is the same for all three configurations. 4000 was chosen as it is
the highest scaling factor that can run in the 128GB of memory of the host
machine without partial materialization.

The figure also shows the lowest memory limit Noria can sustain without when
eviction is enabled. Here, I run Noria with a progressively more aggressive
eviction policy, and stop once the backend can no longer keep up with offered
load. With partial materialization and eviction, Noria uses approximately
20$\times$ less memory for operator state than full materialization does.

\begin{figure}[h]
  \centering
  \includegraphics{graphs/lobsters-memory.pdf}
  \caption{The effect of partial state and the eviction it enables on stateful
  operator memory use. The savings from partial alone stem from many items never
  being accessed. The savings from eviction come from items that are only
  fetched infrequently, and need not be in memory.}
  \label{f:lobsters-memory}
\end{figure}

\section{What is the cost of using partial state?}
\label{s:eval:cost}

Partial state's main drawback compared to traditional materialized views is
that the results for an application's query may not be known. When this happens,
the system must upquery the missing state, which takes time and consumes
resources otherwise dedicated to writes. The fundamental trade-off here is that
of memory use versus tail latency. The more memory you provide the partial
system with, the more of your tail will be pre-computed, and the more of your
requests will be fast. Conversely, you can use less memory if you are willing
for more of your tail to be computed on demand.

Figure~\ref{f:lobsters-mem-latency} explores this trade-off in more detail in
the context of the Lobsters experiment from the previous section. You can see
that as the memory limit shrinks, the fraction of requests in the tail that must
be computed increases, and thus tail latency increases.

\begin{figure}[h]
  \centering
  \includegraphics{graphs/lobsters-memlimit-cdf.pdf}
  \caption{CDF of Lobsters page sojourn latency in steady-state at 3000 pages
  per second as the memory use budget shrinks. The CDF is across all different
  page requests.}
  \label{f:lobsters-mem-latency}
\end{figure}

\begin{inprogress}
  This shows a memory limit far lower than 400MB. Why is memory use still 400MB?
\end{inprogress}

The figure also demonstrates how much costlier upqueries are than cached
reads. Where a read that hits in cache only has to wait for serialization, an
upquery must wait for the requested data to be fetched and computed before it
finishes. If this happens to multiple queries the application issues for a
particular page to be rendered, that latency accumulates, increasing the tail
latency.

\begin{inprogress}
  Why on earth is tail latency here 10 \textbf{seconds}?
  Note that the actual tail latency distribution looks a little
  different\,---\,it's the sojourn time that gives the tail the curve towards
  the max. If you instead look at processing time, only a very small fraction of
  requests take that long, but later requests get queued up behind them (256
  in-flight?)!
\end{inprogress}

The figure above includes data across all the different Lobsters pages the
client requests over the course of the benchmark.
Figure~\ref{f:lobsters-pages-latency} breaks this down by four fairly
different page request types: viewing a story (most common), viewing the front
page, voting for a comment, and submitting a new story (least common).

\begin{figure}[h]
  \centering
  \includegraphics{graphs/lobsters-pages-cdf.pdf}
  \caption{CDF of Lobsters page latency in steady-state at 3000 pages per second
  for different pages without a memory limit. This does \textbf{not} measure
  queueing time.}
  \label{f:lobsters-pages-latency}
\end{figure}

\begin{inprogress}
  When I get the tail under control, the horizontal distance here is
  interesting, since it indicates fewer/more queries are issued. The tail is
  also be interesting, since it indicates how affected the given page is to
  evictions.
\end{inprogress}

Even at a scale factor of 4000$\times$, Lobsters only serves about 3000 pages
per second. While each page executes multiple queries, the absolute number of
queries per second is fairly low, which allows the system to get away with much
of the tail computed on-demand. To shed more light on the extremes of this
trade-off, I next present results for a simplified version of one particular
query from Lobsters: counting the number of votes for an article. The SQL
equivalent of this query is:

\begin{verbatim}
SELECT articles.*, COUNT(votes.user_id)
FROM articles
LEFT JOIN votes ON (articles.id = votes.article_id)
GROUP BY articles.id
WHERE articles.id = ?
\end{verbatim}

Figure~\ref{f:vote-mem-latency} shows a similar latency CDF as for Lobsters, but
for a benchmark stressing this particular query (``vote''). The benchmark issues
requests distributed as 95\% reads and 5\% writes (inserts into \texttt{votes}).
The access pattern is skewed (Zipf; $\alpha = 1.08$) across 5M articles. The
figure demonstrates the trade-off clearly. As you lower the memory limit, more
requests must be computed, and tail latency suffers.

\begin{figure}[h]
  \centering
  \includegraphics{graphs/vote-memlimit-cdf.pdf}
  \caption{CDF of vote read request latency in steady-state at 800k requests per
  second as the memory use budget shrinks.}
  \label{f:vote-mem-latency}
\end{figure}

\begin{inprogress}
  Writes are \textit{consistently} \textbf{much} slower with partial than
  without. That seems like a problem that I should look into.
\end{inprogress}

\begin{inprogress}
  Interestingly, overall latency improves. I though this might be due to
  improved CPU cache utilization with fewer keys in the map (since this
  effectively is a hashmap benchmark), but that doesn't explain why `full`
  (which has all the keys) does so well. Maybe it's because partial must store
  tombstones?
\end{inprogress}

Another important aspect of the partial trade-off is that as throughput
increases, so must the memory budget. The intuition behind this is that the
memory budget effectively dictates your hit rate. The more requests are issued
per second, the more misses (in absolute terms) result from a given hit
fraction. If those misses in the tail are distinct, Noria must satisfy
\textbf{more} upqueries as load increases, while also handling that added load.

Figure~\ref{f:vote-throughput-limit} demonstrates this effect in practice. It
shows throughput-latency lines for the vote benchmark for different memory
budgets. Each point along each line is a higher offered load; its x-coordinate
is the achieved throughput, and its y-coordinate is the measured median latency.
When the backend no longer keeps up, you see a ``boomerang'', where
achieved throughput no longer increases, while latency spikes. The figure
shows that as the offered load increases, Noria needs a larger memory budget to
keep up.

\begin{figure}[h]
  \centering
  \includegraphics{graphs/vote-throughput-memlimit.pdf}
  \caption{Achieved throughput vs median request latency in vote with different
  memory budgets. Offered load increases along the points on each line. At
  higher offered load, a higher memory budget is needed to keep up, or too many
  misses must be handled.}
  \label{f:vote-throughput-memlimit}
\end{figure}

\subsection{Migrations}
\label{s:eval:cost:mig}

When the application issues a query that Noria has never seen before, Noria must
instantiate the dataflow for that query, along with any materializations it
might need.

Partial state also enables efficient migrations in many cases. Any view
that can be made partial (ref section + later eval) is immediately
available, and does not compute until requested.

\begin{figure}[h]
  \centering
  \includegraphics{graphs/vote-migration.pdf}
  \caption{Partial. Reuse. Skewed. 10M articles.}
  \label{f:vote-migration}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics{graphs/vote-migration-full.pdf}
  \caption{Full. Reuse. Skewed. 10M articles.}
  \label{f:vote-migration-full}
\end{figure}

\begin{inprogress}
  Should partial run with eviction here?
  Would mean that write performance does not degrade over time!
\end{inprogress}

Upqueries are particularly frequent immediately after a new view is added, since
it starts out entirely empty.

Fig.: Latency timeline to show that early accesses are slow.

\begin{inprogress}
Maybe show vote uniform numbers? Talk about ``warmup''?
\end{inprogress}

\section{How dependent is partial state on application access patterns?}
\label{s:eval:patterns}

Partial state is mainly useful if accesses are skewed towards particular queries
and particular data. When this is the case, caching a small number of the
application's computed state speeds up a significant fraction of requests. If
this is not the case, the likelihood of missing in the cache is inversely
proportional to the size of the cache, and you would need to cache most of the
data to have a decent cache hit rate.

\begin{inprogress}
The memory use for partial with and without eviction from earlier suggest that
this skew is indeed present in normal applications. As do other papers on the
subject (ref such papers).
\end{inprogress}

\begin{inprogress}
Note that eviction is necessary because, over time, more and more of the
tail will be sampled. Without eviction, eventually the entire tail would
be kept in memory (so == full).
\end{inprogress}

Deciding what memory limit to set is challenging, in part because it
may change over time as the access patterns change, and in part because
it depends on load. The higher the load, the more requests will be in
the tail (and miss). Of course, more requests will be in the head too.


For example, for Zipf distribution, we can compute \%s:

Tab.: | load | skew | keys hit in 30s | keys hit by 99\% | 

This is (again) why eviction is important — without it, partial would approach
full.

Lobsters suggests that significant skew is common. Also ref other
studies on log-normal/zipf/skew.

\subsection{When can partial state be used?}

\begin{itemize}
 \item SELECT COUNT(*) FROM table;
 \item Add ``karma'' query:
   \begin{itemize}
    \item If already have ``karma'' column, trivial in DB. But, equivalent in
      Noria to saying that query was there from the start, so no migration.
    \item If \textbf{not} already have ``karma'' column, DB has two choices:
     \begin{enumerate}
      \item Compute column == expensive, and same as Noria would do
      \item Make query compute value on-demand -- same cost, but repeated!
     \end{enumerate}
   \end{itemize}
 \item TopK
\end{itemize}

\section{How does partial state compare to existing solutions?}
\label{s:eval:existing}

This is a difficult question to answer. High-performance solutions are
often developed specifically for a given application, and not available
as general-purpose tools (ref facebook memcache, need cites here).
And applying the general-purpose tools that \textbf{are} available (memcache,
redis) effectively, requires significant effort on the part of the
application authors (or the evaluators). To manually add caching support
to Lobsters' XX queries, including mitigation, thundering herd
mitigation, and incremental updates would be a massive undertaking.

In some sense, this alone is an argument for Noria's approach. Since the
database uses information it already possesses (in the form of
application queries) to automatically optimize accesses through
materialization, it is relatively easy to take advantage of the benefits
that Noria provides.

Nonetheless, to shed some light on the absolute performance that Noria
provides against a caching system, I include below a performance
comparison between Noria and Redis. To approximate how a carefully
planned and optimized application caching deployment might perform, it
runs a workload that is idealized for a caching system:

\begin{itemize}
 \item Every Redis access hits in cache, to emulate perfect thundering herd
   mitigation and invalidation-avoidance schemes.
 \item Nearly all accesses (99.9\%) are reads, since writes would be
   bottlenecked by the backing store.
 \item All accesses are for a single integer value, to emulate a system that has
   perfect cache coverage. Request access keys are chosen according to a Zipfian
    distribution with $\alpha = 1.08$ (moderate skew).
 \item Accesses are batched to reduce serialization cost and increase
   throughput. Specifically, reads are \texttt{MGET}s, and writes are pipelined
    \texttt{INCRBY}s.
\end{itemize}

This is not a realistic use of Redis, but it allows us to ``assume the best''
about the underlying caching strategy and system. The benchmark runs for four
minutes, and then samples latencies for another two minutes.

\begin{figure}[h]
  \centering
  \includegraphics{graphs/vote-redis.pdf}
  \caption{Throughput vs 90\%-ile latency.}
  \label{f:vote-redis}
\end{figure}

Redis is single threaded, which necessarily limits its performance. If we assume
perfect sharding, Redis should be able to support 16 times the load on a 16-core
machine. We see that 16x Redis ~= Noria (??), which suggests that Noria is
competitive with manual caching schemes.

\begin{inprogress}
  Why on earth is Noria (full) \textbf{so} much faster than Noria (partial)?!
  Maybe this is related to the fact that writes are much slower with partial
  (why?), and we're basically bottlenecked by write performance...
\end{inprogress}

\begin{inprogress}
  Note that the reason vote falls over when it does is single-core write
  processing (since only one query, so one data-flow path).
\end{inprogress}
