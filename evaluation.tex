This thesis is built on the belief that view materialization is useful, but
prohibitively costly to use with current solutions. It presents partial state as
a solution to this problem, which allows retaining the benefits of view
materialization at a fraction of the cost. Section \ref{s:eval:why} evaluates
the validity of this assumption, and the efficacy of partial state as a
solution.

With partial state, only a subset of each view is materialized, and missing
results are computed on-demand. This reduces memory use, but also means that
some queries take a while to be satisfied. Ultimately, partial state presents a
trade-off between memory use (cache size) and tail latency (miss rate). Section
\ref{s:eval:cost} explores the implications of this trade-off.

Partial state allows applications to introduce new views without materializing
that entire view all up-front. Instead, the view is gradually materialized
on-demand. This enables fast adoption of new queries, but also means that
queries to new views are initially slow. Section \ref{s:eval:mig} evaluates such
partial state migrations.

The efficacy of partial state as a solution to the view materialization memory
use problem depends on application access patterns being sufficiently skewed that
keeping only some results cached reduces latency for a significant fraction of
requests. While it is impossible to predict the skew for an arbitrary
application's data and queries, section \ref{s:eval:patterns} gives a simple
theoretical model to help with estimation.

\section{Experimental Setup}
\label{s:eval:setup}

The experiments in this chapter revolve primarily around the Lobsters news
aggregator web application at \url{https://lobste.rs}~\cite{lobsters}. This
application was chosen because it is open-source (so we can see what queries it
issues), because it resembles many larger-scale applications (like Hacker News
or Reddit), and because statistics about the site's data and access patterns are
available~\cite{lobsters-data}.

The thesis evaluation uses a workload generator that issues page requests
according to the available statistics~\cite{generator}. It does not run the
real Lobsters Ruby-on-Rails application, as it is prohibitively slow. Instead,
all experiments use an adapter that turns page requests directly into the
queries the real Lobsters code would issue for that same page request. The
generator supports scaling up the rate of access to emulate a larger user base
for benchmarking.

Experiments run on Amazon EC2 r5n.4xlarge instances, which have 16 vCPUs and
128GB of memory. The server is always given a dedicated host, while load
generating clients are split across one or more m5n.4xlarge instances depending
on the desired load factor. Experiments are run for a bit over 2 minutes unless
otherwise specified.

The benchmark setups are all ``partially open-loop''~\cite{frank-open-loop}:
clients generate load according to a workload-dictated distribution of
interarrival-times, and has a limited number of backend requests outstanding,
queueing additional requests. This ensures that clients maintain the measurement
frequency even during periods of high latency. The test harness measures offered
request throughput and ``sojourn time''~\cite{open-loop-cautionary-tale}, which
is the delay the client experiences from request generation until a response
returns from the backend.

% We seed the database with 9.2k users, 40k stories and 120k comments---the
% size of the real Lobsters deployment---and run increasing request loads to
% push the different setups to their limits.

\section{Benefits and Costs of View Materialization}
\label{s:eval:why}

The core argument of this thesis is that partial state makes view
materialization feasible. Bundled up in that argument are several intertwined
questions that must be answered before further evaluation of partial state is
interesting:

\begin{enumerate}
    \item Why is view materialization desirable?
    \item Why is view materialization not feasible currently?
    \item Does partial state improve on this situation?
\end{enumerate}

\begin{figure}[h]
  \centering
  \includegraphics{graphs/lobsters-memory.pdf}
  \caption{Maximum achieved throughput on Lobsters benchmark with and without
  view materialization. Without view materialization, MySQL must compute query
  results each time. Traditional (full) view materialization runs out of memory
  at $\approx$2.3k pages/second. Partial state allows Noria to reduce memory use
  significantly so that it can sustain higher throughput.}
  \label{f:lobsters-memory}
\end{figure}

Figure~\ref{f:lobsters-memory} attempts to give insight into all these questions
by comparing the highest sustainable request load of three different systems:
MySQL, Noria without partial state, and Noria with partial state. MySQL is run
entirely in RAM by running it on a ramdisk, and on its lowest isolation level.
The figure shows the highest Lobsters throughput each system achieves before its
median latency exceeds 50ms.

View materialization alone (as provided by Noria) improves performance by almost
$6\times$ compared to MySQL, as query results are now frequently cached.
However, without partial state, this performance increase comes at a significant
memory cost. Beyond 2.3k pages/second, Noria runs out of memory, and cannot
support the workload. With partial state, Noria uses only 39GB of memory, a
third less than the 58GB used with full materialization. This in turn allows it
to support 65\% higher throughput, almost 10$\times$ that of MySQL%
\footnote{The Noria benchmarks are memory constrained, not CPU constrained.
MySQL fully loads all 16 cores at 391 pages per second.}.

The memory use reductions with partial state are a direct result of the skew in
Lobsters data popularity and access patterns. Many pages are simply never
visited over the course of the benchmark, and so need not be brought into the
cache. With partial state, Noria also evicts infrequently accessed results,
which further reduces memory use, and ensures that the cache does not eventually
grow to contain all results.

Since partial state uses less memory, applications that do not need higher
throughput can instead reduce cost by purchasing less memory. For example, on
AWS EC2, going from a 64GB instance type with 16 cores (\texttt{m5n.4xlarge}) to
a 128GB instance type with 16 cores (\texttt{r5n.4xlarge}) comes at a 25\% price
increase. Instances with 256GB of memory only come with 32 cores
(\texttt{r5n.8xlarge}) or more, and come out at twice the price of 128GB.

\begin{inprogress}
  These results come with Noria keeping all its results in memory. By offloading
  the storage of base tables to disk instead, memory use can be further reduced.
  In Lobsters, this is unlikely to increase performance by much, as throughput
  it bottlenecked at 3.8k pages/second by updates to a particularly complex
  view.
\end{inprogress}

% The key question for partially stateful dataflow then is whether it truly and
% meaningfully reduces application memory use. Or, more accurately, whether
% applications can run efficiently when only some of its query results are cached.
% Intuitively, one might expect this to be true for many user-driven applications:
% some queries may only be executed for rarely used features, and unpopular posts
% may be accessed only very infrequently, or perhaps not at all, after they were
% initially posted.

\section{The Memory/Latency Trade-off}
\label{s:eval:cost}

Partial state's main drawback compared to complete materialized views is
that the results for an application's query may not be known. Or, stated
differently, some reads may miss. When this happens, the system must upquery the
missing state, which takes time and consumes resources otherwise dedicated to
writes. This shows up as increased tail latency for the application: queries
whose results are not known must wait to be computed. The hope with partial
state is that, once the commonly-accessed query results are cached, latency
quickly drops such that going forward only the infrequently accessed query
results must be computed on-demand.

\subsection{Warming the Cache}

\begin{figure}[t]
  \centering
  \includegraphics{graphs/lobsters-timeline.pdf}
  \caption{Lobsters latency timeline at 1.5k pages per second across all pages
  with eviction. Figure shows the latency profile seen by the client over time,
  starting at the point when the first query is issued. Time increases along the
  x-axis, with each bin sampling twice as long as the previous one. The measured
  latency for each time bin is plotted on a logarithmic scale on the y-axis.
  Progressively lighter colors include more of the tail.}
  \label{f:lobsters-timeline}
\end{figure}

The cost of these misses is particularly visible when Noria starts with empty
state. This is equivalent to starting a more traditional caching system with an
empty (cold) cache, and having to ``warm'' it by filling in the most popular
entries. To measure this warming period, Figure~\ref{f:lobsters-timeline} shows
the latency profile seen by the Lobsters benchmark client over time, starting at
the point when the first query is issued. Time increases along the x-axis, and
the measured latency for each time bin is plotted on a logarithmic scale on the
y-axis. Progressively lighter colors indicate values further into the tail.

The figure shows that latency is initially high, but after a few seconds, the
mean and 95th percentile latency drop below 10 milliseconds. By the time a
minute has passed, the 99th percentile has followed suit. Since only a small
fraction of the total computed state is cached (as shown in the top-left bar of
Figure~\ref{f:lobsters-memory}), this supports the hypothesis that partial state 
achieves low latency once the most commonly accessed results are cached.

\subsection{Impact on Tail Latency}

\begin{figure}[h]
  \centering
  \includegraphics{graphs/lobsters-memlimit-cdf.pdf}
  \caption{CDF of sojourn latency across all Lobsters pages at 3k pages per
  second with different amounts of allocated memory. The figure depicts
  steady-state operation\,---\,the benchmark has been allowed to run for two
  minutes before the latency is measured.}
  \label{f:lobsters-mem-latency}
\end{figure}

Partial's trade-off is that of memory use versus tail latency; as you allocate
less memory to Noria, less of your tail can be pre-computed, and thus more of
your requests will be slow as it must be computed on-demand.
Figure~\ref{f:lobsters-mem-latency} shows this trade-off in the steady state%
\footnote{The benchmark runs for two minutes before latencies are sampled.}
of the application. It plots the CDF of the sojourn latency across all requests
with increasingly aggressive eviction policies. As Noria is asked to
reduce memory use by evicting more aggressively (darker purple lines), more
requests take longer, and the tail grows. In other words, the further you want
to reduce memory use, the more you pay in latency.

The reason why the whole curve shifts, rather than just the tail, is that this
CDF is across \emph{all} the different page types in Lobsters. Each one issues a
different set of queries, and so their total time differs, as does the effect of
a longer tail. The exact shape of this curve, and how it shifts in response to
varying resources, depends on the application in question.
\S\ref{s:eval:patterns} attempts to give a useful approximation heuristic for
other applications.

\begin{inprogress}
Full materialization is slightly faster than partial materialization because
SOMETHING SOMETHING either join eviction or faster lookups due to no tombstones?
\end{inprogress}

% \begin{figure}[h]
%   \centering
%   \includegraphics{graphs/lobsters-pages-cdf.pdf}
%   \caption{CDF of Lobsters page latency in steady-state at 3000 pages per second
%   for different pages without eviction. This does \textbf{not} measure queueing
%   time.}
%   \label{f:lobsters-pages-latency}
% \end{figure}
%
% The figure above includes data across all the different Lobsters pages the
% client requests over the course of the benchmark.
% Figure~\ref{f:lobsters-pages-latency} breaks this down by four fairly
% different page request types: viewing a story (most common), viewing the front
% page, voting for a comment, and submitting a new story (least common).

The memory use can only be reduced so far before the system can no longer keep
up with the offered load. If some of the most frequently accessed query results
are not cached, the system will constantly have to re-compute those results to
satisfy reads that come in after the eviction, which increases latency, and
decreases throughput. This effect then compounds for each popular query result,
as each one is accessed frequently. Essentially, the system will never finish
warming the cache, and latency will remain at the high levels shown early in
Figure~\ref{f:lobsters-timeline}. For Lobsters, this happens around the 150MB
mark. If the eviction is tuned to evict operator state below that threshold,
Noria can no longer sustain 3.8k pages per second.

\subsection{Memory Use and Throughput}

Generally speaking, as throughput increases, so must the memory budget. The
intuition behind this is that the memory budget effectively dictates your hit
rate. The more requests are issued per second, the more misses (in absolute
terms) result from a given hit fraction. If those misses in the tail are
distinct, Noria must satisfy \textbf{more} upqueries as load increases, while
also handling that added load.

\begin{listing}[h]
  \begin{minted}{sql}
    SELECT articles.*, COUNT(votes.user_id)
    FROM articles
    LEFT JOIN votes ON (articles.id = votes.article_id)
    GROUP BY articles.id
    WHERE articles.id = ?
  \end{minted}
  \caption{Simplified query for vote counting in Lobsters.}
  \label{l:votes}
\end{listing}

While this correlation between throughput and memory use exists in Lobsters, it
is difficult to show clearly as each page issues many different queries, and
overall load is relatively low. For this reason, the next set of benchmarks use
a simplified version of one particular query from Lobsters shown in
Listing~\ref{l:votes}. It counts the number of votes for an article, and
presents that alongside the article information. The benchmark issues requests
distributed as 99\% reads and 1\% writes (inserts into \texttt{votes}). The
access pattern is skewed such that 90\% of requests access 1\% of keys across 5M
articles%
\footnote{The benchmark samples keys from a Zipfian distribution with a skew
factor ($\alpha$) of 1.15}.

\begin{figure}[h]
  \centering
  \includegraphics{graphs/vote-throughput-memlimit.pdf}
  \caption{Achieved throughput vs 90th percentile request latency in vote with
  different memory budgets. Offered load increases along the points on each
  line. At higher offered load, a higher memory budget is needed to keep up, or
  too many misses must be handled.}
  \label{f:vote-throughput-memlimit}
\end{figure}

Figure~\ref{f:vote-throughput-memlimit} demonstrates the connection between
throughput and memory use. It shows throughput-latency lines for the vote
benchmark for different memory allowances. Each point along each line is a
higher offered load; its x-coordinate is the achieved throughput, and its
y-coordinate is the measured median latency. When the backend no longer keeps
up, you see a ``hockey stick'' effect, where achieved throughput no longer
increases, while latency spikes. The figure shows that as the offered load
increases, Noria needs progressively more memory to keep up.

\section{Bringing Up New Views}
\label{s:eval:mig}

When the application issues a query that Noria has never seen before, Noria must
instantiate the dataflow for that query, along with any materializations it
might need. With full materialization, the system must do all the work to
compute the full state for the new view, and any internal operator state it
depends on, up front and all at once. And during that time, Noria's dataflow
must spend cycles on computing that new state, slowing down the processing of
other concurrent writes. The new view also cannot serve any reads until all the
state is computed.

\begin{listing}[h]
  \begin{minted}{sql}
    CREATE VIEW scores AS
      -- same vote count (each vote is a 1 rating)
      SELECT votes.article_id, COUNT(votes.user_id) AS score
      FROM votes
      GROUP BY votes.article_id
    UNION
      -- compute total rating score
      SELECT ratings.article_id, SUM(ratings.rating) AS score
      FROM ratings
      GROUP BY ratings.article_id;

    -- new view aggregates across votes and ratings
    SELECT articles.*, SUM(scores.score)
    FROM articles
    LEFT JOIN scores ON (articles.id = scores.article_id)
    GROUP BY articles.id
    WHERE articles.id = ?;
  \end{minted}
  \caption{Updated query for ``rating'' counting in Lobsters.}
  \label{l:ratings}
\end{listing}

Partial state enables such migrations to be instantaneous in many cases\,---\,if
the new view can be made partial it is instantiated as empty, and immediately
made available, and will be filled on demand as the application submits reads.
To demonstrate the different behavior of full and partial materialization for
migrations, the next benchmark makes a modification to the ``vote'' benchmark
illustrated by Listing~\ref{l:votes}. It introduces a new table,
\texttt{ratings}, which has ``ratings'' on a scale from 0 to 1 for each article
instead of just a binary 0 or 1. It also add a new view, shown in
Listing~\ref{l:ratings}, which combines the existing votes with the new ratings
to compute a total article score%
\footnote{By writing the query this way, votes and ratings can co-exist.}.

The benchmark inserts votes and issues the original vote query
(Listing~\ref{l:votes}) for 90 seconds, and then introduces the new table and
query (denoted as time 0). From then on, it issues both votes and ratings, and
queries both views without blocking every 10 milliseconds.

\begin{figure}[h]
  \centering
  \includegraphics{graphs/vote-migration.pdf}
  \caption{Time to set up and access a new view. Access pattern is skewed (Zipf;
  $\alpha = 1.15$) across 10M articles. Benchmark runs for 90s prior to
  migration (solid red line). The dashed vertical line denotes the end of the
  migration for full materialization.}
  \label{f:vote-migration}
\end{figure}

Figure~\ref{f:vote-migration} plots the cache hit rate for reads from the new
view over time, as well as the write throughput over the course of the
experiment. With full materialization, the new view is not accessible until its
construction finishes after $\approx$23 seconds, and during that time, the
application write performance drops substantially. With partial materialization,
the view is immediately accessible, though its cache hit rate is initially low.
However, since there are a few very popular keys, the hit rate quickly climbs to
over 90\%. Since only results for requested keys are computed, write throughput
is mostly unaffected by the low upquery rate.

The figure also exposes another interesting effect of using partial
materialization: increased write throughput. With full materialization, every
write must be processed to completion, since all results are cached. With
partial state, writes for keys that have not been read can be discarded early,
as there is no state in memory that must be updated, which increases throughput.

% Figure~\ref{f:vote-migration} shows that, for the vote query, the hit rate is
% 85\% after about 20s, but this is a very limited sample. To get a better idea of
% how this time varies for a wider variety of situations, we return to the
% Lobsters experiment.
% 
% \begin{figure}[t]
%   \centering
%   \includegraphics{graphs/vote-timeline.pdf}
%   \caption{Vote read latency timeline at 800k operations per second without
%   eviction. \textbf{Both axes use a logarithmic scale.} Brighter colors include
%   more of the tail. Note that y axis is one order of magnitude lower than for
%   the Lobsters plots.}
%   \label{f:vote-timeline}
% \end{figure}

\section{Skew}
\label{s:eval:patterns}

Partial state is mainly useful if accesses are skewed towards a particular
subset of queries and data. When this is the case, caching a small number of the
application's computed state speeds up a significant fraction of requests. If
this is not the case, the likelihood of missing in the cache is inversely
proportional to the size of the cache, and you would need to cache computations
over most of the data to maintain a decent cache hit rate.

Significant skew shows up across a wide range of real-world
datasets~\cite{power1,power2,network-skew}, including many social
networks~\cite{network-skew2, community-skew}. In a large public Amazon
dataset~\cite{amazon-skew}, the 100,000 most popular book titles (less than 5\%)
account for roughly 50\% of all book sales, and 75\% of the sales are for the
top 500,000 titles~\cite{zhang2020permutation}. In the Millennium
simulation~\cite{large-skew}, an important astrophysical data set that contains
more than 18 million trees, mass distribution among the trees is highly skewed,
with the 7 most frequent values appearing over 20 million times each, while
almost 75\% of the values appear no more than 10
times~\cite{large-skew-analysis}. This kind of significant skew also appears in
Lobsters, which is what allows it to run smoothly even when only a small
fraction of results are cached.

The degree of skew dictates how aggressively you can reduce the amount of memory
used by partial state. But it can be difficult to estimate this ahead of time
for a complex application. The Zipf distribution~\cite{zipf} used in the vote
benchmark provides what may be a useful heuristic, since it allows
probabilistically computing the fraction of the data set that is accessed over a
period of time. Given some number of elements $N$ and a skew parameter $\alpha$,
the normalized frequency of the $k$th element in a Zipf distribution is given by

\begin{displaymath}
  f(k;\alpha,N)={\frac {1/k^{\alpha}}{\sum \limits _{n=1}^{N}(1/n^{\alpha})}}
\end{displaymath}

Every time the benchmark performs a read or a write, it samples a value from
this distribution. Zipf distributes the values such that values at lower $k$ are
more frequently sampled than those at higher $k$. How frequently is dictated by
the $\alpha$ parameter. The expected number of keys hit after a certain number
of samples, $S$ (given by throughput $\times$ period), is then the sum of the
probability that each element $k$ is sampled at least once during the benchmark,
given by

\begin{displaymath}
  F(\alpha,N)={\sum \limits _{k=1}^{N} \left(1 - \left(1 - f(k; \alpha, N)\right)^{S}\right)}
\end{displaymath}

\begin{figure}[h]
  \centering
  \includegraphics{graphs/vote-formula.pdf}
  \caption{Probabilistic model of the fraction of 5M keys that are accessed
  (``hot'') during each eviction period ($\approx$ 2s) as throughput increases.
  Each line shows a different amount of skew. Skew (X/Y) denotes that X\% of
  requests come from Y\% of keys. More keys pushes the curve down. With good
  eviction, memory use will follow this curve.}
  \label{f:vote-formula}
\end{figure}

Figure~\ref{f:vote-formula} plots $F(\alpha, N)/N$ after one eviction cycle
($\approx$ 2s) for different degrees of skew ($\alpha$) with $N=5\text{M}$ as
throughput varies. That value corresponds to the expected fraction of keys
accessed between any two eviction cycles, and effectively sets a lower bound on
the fraction of the query results that must be cached. It thus also dictates
minimum memory use. While Noria \emph{could} maintain a smaller fraction of the
query results, the application would likely need those keys again shortly after
evicting them, and Noria would continuously compute and then discard frequently
accessed query results.

In practice, Noria can rarely get quite as low as the graph indicates. Eviction
is performed on the write path, so if the workload is write-bound, the system
may not keep up if eviction happens as aggressively as is needed to reach these
bounds. For example, at 800k requests/s with 90/1 skew, the model predicts that
only $\approx3.5$\% of keys need be materialized, but Noria can only get to
$\approx28$\% of the fully materialized cache size before write latency
increases dramatically.

\begin{inprogress}
  We have a couple of options here:
    1) try to find ways to optimize eviction;
    2) lengthen the eviction interval;
    3) run with lower skew at lower throughput; or
    4) run with a lower write rate (currently 99/1).
  None of these are ideal, but 3.5\% vs 28\% is also not particularly
  compelling. We could run at lower load, but then the formula number also
  lowers comparatively.
\end{inprogress}

\section{Rolling Your Own}
\label{s:eval:existing}

Many applications already require lower latency and higher throughput than
straightforward SQL queries against traditional relational databases can
provide. Developers often go through a sequence of manual optimizations to try
to mitigate this.

The first step is usually to pre-compute certain computed values and store them
directly in the database. For example, in Lobsters, each story has a
``hotness'', which is a score of how popular a story is, and thus how far up it
should appear in listings. This value depends on a lot of parameters, such as
the number of votes, the number of comments, etc. It would be prohibitively
expensive to compute a story's hotness directly in the queries,
especially in the context of computing the front page view, since it requires
the hotness for \emph{all}. Instead, the Lobsters developers add a computed
column, \texttt{hotness}, to the \texttt{story} table. This column is then
updated whenever relevant data changes, such as when:

\begin{itemize}
    \item a story is upvoted or downvoted.
    \item a comment is added to a story.
    \item a comment on a story is upvoted or downvoted.
    \item a comment or vote is deleted.
    \item one story is merged into another.
\end{itemize}

There are several such computed columns in Lobsters, and all write paths must
ensure that they correctly update all related computed values. This process is
manual and error-prone. With Noria, this kind of manual manual materialization
is not necessary\,---\,Noria performs and maintains relevant materializations as
needed.

If database optimizations do not suffice for the application's performance
needs, developers usually add a cache in front of the database. This cache
often takes the form of a key-value store which holds frequently accessed,
computed results. When it issues a query, the application checks the (fast)
cache first, and only if the results are not available in cache is the backend
consulted.

A dedicated cache speeds up reads that hit, but introduces significant
application complexity. Just like for manual query optimizations, all parts of
the application that modify data related to any given cache entry must know to
also invalidate or update the cache. In addition, the developers must ensure
that if multiple clients miss on a given entry, they do not hit the backend
database all at once. This is especially important if a popular entry is
invalidated, as it may cause a ``thundering herd'' effect where a large number
of clients swarm the backend and overwhelm it. Furthermore, since the clients
must now access two separate systems, mechanisms must be in place to ensure that
the cache remains consistent with the underlying data. This is difficult since
data may be updated at any time, including just after a client has fetched the
(then) latest data from the database to update the cache.

Implementing caching ``correctly'' requires highly sophisticated
machinery~\cite{facebook-memcache, transactional-cache, orm-cache, sql-cache}
which developers may not even think to employ. A survey from 2016 found that
0.3-3.0\% of application code spread across 2.1-10.8\% of the application's
source files is caching-related, and that cache-related issues make up 1-5\% of
all issues~\cite{caching-is-hard}. Since Noria integrates the cache with the
database, it relieves the application from considering these problems \,---\,the
cache is maintained directly by the database, and the application code needs no
modifications to gain the benefits of caching.

Despite how error-prone the approach is, ad-hoc, per-application caching is
still common in practice. And while Noria eliminates most of the developer
burden of getting caching right, it must offer competitive performance with
manually constructed caching solutions to present a viable alternative.
Unfortunately, this is difficult to evaluate, since high-performance solutions
are often developed specifically for a given application, and not available as
general-purpose tools. And effectively applying the general-purpose tools that
\textbf{are} available, like memcache and redis, requires significant effort on
the part of the application authors (or the evaluators). To manually add caching
support to Lobsters' ~80 queries, including thundering herd mitigation and
incremental updates, would be a massive undertaking.

In an attempt to approximate how a carefully planned and optimized application
caching deployment might perform, the next experiment runs the vote benchmark
against Redis~\cite{redis} with the following modifications:

\begin{itemize}
 \item Every access hits in cache, to emulate perfect thundering herd mitigation
   and invalidation-avoidance schemes.
 \item Nearly all accesses (99.9\%) are reads, since writes would be
   bottlenecked by the backing store.
 \item All accesses are for a single integer value, to emulate a system that has
   perfect cache coverage. Request access keys are chosen according to a Zipfian
    distribution with $\alpha = 1.15$, so that 1\% of keys make up 90\% of
    requests.
 \item Data is not stored anywhere except Redis.
 \item Accesses are batched to reduce serialization cost and increase
   throughput. Specifically, reads are \texttt{MGET}s, and writes are pipelined
    \texttt{INCRBY}s.
\end{itemize}

This is not a realistic use of Redis as a cache, and ignores the complexities of
integrating the cache with the application, but it allows us to ``assume the
best'' about the underlying caching strategy and system. The benchmark runs for
four minutes, and then samples latencies for another two minutes.

\begin{figure}[h]
  \centering
  \includegraphics{graphs/vote-redis.pdf}
  \caption{Achieved throughput vs 90th \%-ile request latency in cache-optimized
  vote. Offered load increases along the points on each line. Vertical
  lines are $16\times$ the cliff of the corresponding experiment.}
  \label{f:vote-redis}
\end{figure}

Figure~\ref{f:vote-redis} shows a throughput-latency plot that explores the
performance profiles of Redis and Noria under these experimental conditions%
\footnote{Noria runs with the same modified access patterns as outlined for
Redis.}. For comparison, it also includes a MySQL + Redis implementation that
stores votes and articles in MySQL, and uses the na\"ive write-back cache
strategy outlined above. Since Redis is not multi-threaded, and so can only use
one of the server's 16 cores, the figure also includes the Redis performance
extrapolated to 16 cores. To achieve this performance in practice, the
application's already-perfect caching scheme would also need to shard perfectly.

Due to the simplifications in this particular benchmark, the experimental
results serve primarily to determine upper bounds for possible performance:
Noria achieves 25\% of the theoretical best you could ever do with Redis. Since
the caching is unlikely to be perfect as in these experiments, Noria's
performance is likely to be close to what a ``real'' caching system achieves.
And Noria achieves this performance while providing rich SQL queries without
application-specific caching logic.
