This thesis is built on the belief that view materialization is useful, but
prohibitively costly to use with current solutions. It presents partial state as
a solution to this problem that allows retaining the benefits of view
materialization at a fraction of the cost. Section \ref{s:eval:why} evaluates
the validity of this assumption, and the efficacy of partial state as a
solution.

Existing application deployments tend to implement their own ad-hoc caching
logic, usually by placing a dedicated cache in front of the database. Some
larger companies have also built comprehensive tooling to manage their caches
correctly. Section \ref{s:eval:alts} looks at how Noria serves as an attractive
alternative to these approaches.

With partial state, only a subset of each view is materialized, and missing
results are computed on-demand. Partial state thus presents a trade-off between
memory use (cache size) and tail latency (miss rate). Section \ref{s:eval:cost}
explores the effects of this trade-off.

Developers may be hesitant to switch applications that already use a cache today
to Noria without some evidence that their performance won't regress.
Unfortunately, the impact Noria would have on any given application is highly
dependent on the particulars of each application, and is thus hard to measure.
To demonstrate that Noria can offer comparable absolute performance to existing
caching solutions, section \ref{s:eval:kvperf} compares Noria lookup performance
to that of Redis under ideal caching conditions.

An added benefit of partial state is that it allows applications to introduce
new queries/views without materializing it entirely up-front. This enables fast
adoption of new queries, but also means that queries to new views are initially
slow. Section \ref{s:eval:mig} evaluates these partial state migrations compared
to traditional all-at-once materialization.

The ability of partial state to reduce the memory use of view materialization
depends on skew in the application's data and access patterns. It allows Noria
to reduce latency for a significant fraction of requests by keeping only a few
results cached. While it is impossible to predict the skew for an arbitrary
application's data and queries, section \ref{s:eval:patterns} gives a simplified
theoretical model to help with estimation.

\section{Experimental Setup}
\label{s:eval:setup}

The experiments in this chapter revolve primarily around the Lobsters news
aggregator web application at \url{https://lobste.rs}~\cite{lobsters}. This
application was chosen because it is open-source (so we can see what queries it
issues), because it resembles many larger-scale applications (like Hacker News
or Reddit), and because statistics about the site's data and access patterns are
available~\cite{lobsters-data}.

The thesis evaluation uses a workload generator that issues page requests
according to the available statistics~\cite{generator}. It does not run the
real Lobsters Ruby-on-Rails application, as it is prohibitively slow. Instead,
all experiments use an adapter that turns page requests directly into the
queries the real Lobsters code would issue for that same page request. The
generator supports scaling up the rate of access and user count to emulate a
larger user base for benchmarking.

The various pages in Lobsters differ significantly in what queries they issue,
how many queries they issue, and the extent to which they are read or write
heavy. Just over half of all requests are for the pages of individual stories,
which fetches the comments for that story, along with its popularity score and
the score of each associated comment. The story page is mostly read-only, and
issues 12 read queries. Each request also performs a single update to keep track
of when the user last viewed the story for notification purposes. A third of
requests are for the front page, which lists the 25 most highly scored stories,
along with their scores. The front page is read-only, and also issues 12
queries. The most write-heavy pages are the ones for voting on a story or
comment, or adding a new comment. These operations are rare, and make up only
about 1 in every 60 requests. They also issue much fewer queries: two for votes
and five for comments. Latency is measured across all requests, no matter what
page they are for.

Experiments run on Amazon EC2 r5n.4xlarge instances, which have 16 vCPUs and
128GB of memory. The server is always given a dedicated host, while load
generating clients are split across one or more m5n.4xlarge instances depending
on the desired load factor.

The benchmark setups are all ``partially open-loop''~\cite{frank-open-loop}:
clients generate load according to a workload-dictated distribution of
interarrival-times, and has a limited number of backend requests outstanding,
queueing additional requests. This ensures that clients maintain the measurement
frequency even during periods of high latency. The test harness measures offered
request throughput and ``sojourn time''~\cite{open-loop-cautionary-tale}, which
is the delay the client experiences from request generation until a response
returns from the backend.

All experiments measure memory use using the resident virtual memory of the
server process (VmRSS). This measurement therefore includes all indexes, runtime
allocations, and other bookkeeping metadata. For Noria, it also includes the
data stored in the base tables.

Since the benchmarks introduce more data as they run, memory use increases over
the course of each run. Experiments are run for a bit over 5 minutes unless
otherwise specified, and memory measurements are taken at the end of the run.

\section{Benefits and Costs of View Materialization}
\label{s:eval:why}

The core argument of this thesis is that partial state makes view
materialization feasible. Bundled up in that argument are several intertwined
questions that must be answered before further evaluation of partial state is
interesting:

\begin{enumerate}
    \item Why is view materialization desirable?
    \item Why is view materialization not feasible currently?
    \item Does partial state improve on this situation?
\end{enumerate}

\begin{figure}[h]
  \centering
  \includegraphics{graphs/lobsters-throughput.pdf}
  \caption{Maximum achieved throughput on Lobsters benchmark with and without
  view materialization. Without view materialization, MySQL must compute query
  results each time. Traditional (full) view materialization runs out of memory
  at $\approx$4.6k pages/second. Partial state allows Noria to reduce memory use
  significantly so that it can achieve higher throughput.}
  \label{f:lobsters-throughput}
\end{figure}

Figure~\ref{f:lobsters-throughput} attempts to explain why view materialization
is desirable. It compares the highest sustainable request load of three
different systems: MySQL, Noria without partial state, and Noria with partial
state. MySQL is run entirely in RAM by running it on a ramdisk, and on its
lowest isolation level. The figure shows the highest Lobsters throughput each
system achieves before its mean latency exceeds 50ms.

View materialization alone (as provided by Noria) improves performance by almost
$12\times$ compared to MySQL, as query results are now frequently cached.
However, without partial state, this performance increase comes at a significant
memory cost. Beyond 4.6k pages/second, Noria runs out of memory, and cannot
support the workload. With partial state, Noria uses much less memory at a given
load factor, which allows it to support 67\% higher throughput, almost
20$\times$ that of MySQL%
\footnote{The Noria benchmarks are memory constrained, not CPU constrained.
MySQL fully loads all 16 cores at 391 pages per second.}.

\begin{figure}[h]
  \centering
  \includegraphics{graphs/lobsters-memory.pdf}
  \caption{Memory use two minutes into the Lobsters benchmark at 4.6k pages per
  second. Striped bars store base tables on disk using RocksDB.}
  \label{f:lobsters-memory}
\end{figure}

Figure~\ref{f:lobsters-memory} shows the memory use at 4.6k pages per second
with and without partial state. It demonstrates both the issues with full
materialization, and the improvements brought about by partial state. With full
materialization, Noria must store every result for every query in memory. In
contrast, with partial state, Noria need only store frequently accessed results,
which cuts memory use in half.

The memory use reductions with partial state are a direct result of the skew in
Lobsters data popularity and access patterns. Many pages are simply never
visited over the course of the benchmark, and so need not be brought into the
cache. With partial state, Noria also evicts infrequently accessed results,
which further reduces memory use, and ensures that the cache does not eventually
grow to contain all results.

Much of Noria's memory use goes to storing the base tables in memory. Since
partial state cannot evict base table state, this limits how much memory can
potentially be saved. The figure therefore also includes memory use when running
Noria with its durable RocksDB storage backend for base tables. In that
configuration, base tables are kept on disk, not in memory, which makes the
memory savings from partial state more apparent\,---\,the memory use is now
about a third that without partial state%
\footnote{Various other runtime overheads that partial state cannot eliminate
remain, such as in-flight requests and pending responses. With diligent memory
optimization, this overhead could likely be further reduced, further increasing
the relative benefits from partial state.}.

Since partial state uses less memory, applications that do not need higher
throughput can instead reduce cost by using hosts with less memory. For example,
on AWS EC2, going from a 64GB instance type with 16 cores (\texttt{m5n.4xlarge})
to a 128GB instance type with 16 cores (\texttt{r5n.4xlarge}) comes at a 25\%
price increase. Instances with 256GB of memory only come with 32 cores
(\texttt{r5n.8xlarge}) or more, and come out at twice the price of 128GB.

\section{Rolling Your Own}
\label{s:eval:alts}

Many applications already require lower latency and higher throughput than
straightforward SQL queries against traditional relational databases can
provide. Developers often go through a sequence of manual optimizations to try
to mitigate this.

The first step is usually to pre-compute certain computed values and store them
directly in the database. For example, in Lobsters, each story has a
``hotness'', which is a score of how popular a story is, and thus how far up it
should appear in listings. This value depends on a lot of parameters, such as
the number of votes, the number of comments, etc. It would be prohibitively
expensive to compute a story's hotness directly in the queries,
especially in the context of computing the front page view, since it requires
the hotness for \emph{all} stories. Instead, the Lobsters developers add a
computed column, \texttt{hotness}, to the \texttt{story} table. This column is
then updated whenever relevant data changes, such as when:

\begin{itemize}
    \item a story is upvoted or downvoted.
    \item a comment is added to a story.
    \item a comment on a story is upvoted or downvoted.
    \item a comment or vote is deleted.
    \item one story is merged into another.
\end{itemize}

There are several such computed columns in Lobsters, and all write paths must
ensure that they correctly update all related computed values. This process is
manual and error-prone.

If database optimizations do not suffice for the application's performance
needs, developers usually add a cache in front of the database. This cache
often takes the form of a key-value store which holds frequently accessed,
computed results. When it issues a query, the application checks the (fast)
cache first, and only if the results are not available in cache is the backend
consulted.

A dedicated cache speeds up reads that hit, but introduces significant
application complexity. Just like for manual query optimizations, all parts of
the application that modify data related to any given cache entry must know to
also invalidate or update the cache. In addition, the developers must ensure
that if multiple clients miss on a given entry, they do not hit the backend
database all at once. This is especially important if a popular entry is
invalidated, as it may cause a ``thundering herd'' effect where a large number
of clients swarm the backend and overwhelm it. Furthermore, since the clients
must now access two separate systems, mechanisms must be in place to ensure that
the cache remains consistent with the underlying data. This is difficult since
data may be updated at any time, including just after a client has fetched the
(then) latest data from the database to update the cache.

Implementing caching ``correctly'' requires highly sophisticated
machinery~\cite{facebook-memcache, transactional-cache, orm-cache, sql-cache}
which developers may not even think to employ. A survey from 2016 found that
0.3-3.0\% of application code spread across 2.1-10.8\% of the application's
source files is caching-related, and that cache-related issues make up 1-5\% of
all issues~\cite{caching-is-hard}.

\textbf{With Noria, these kinds of manual materialization techniques are not
necessary.} Noria performs and maintains the relevant materializations as
needed, and the application needs no cache-related code.

\section{The Memory/Latency Trade-off}
\label{s:eval:cost}

Partial state's main drawback compared to complete materialized views is
that the results for an application's query may not be known. Or, stated
differently, some reads may miss. When this happens, the system must upquery the
missing state, which takes time and consumes resources otherwise dedicated to
writes. This shows up as increased tail latency for the application: queries
whose results are not known must wait to be computed. The hope with partial
state is that, once the commonly-accessed query results are cached, latency
quickly drops such that going forward only infrequently accessed query results
must be computed on-demand.

\subsection{Warming the Cache}

\begin{figure}[t]
  \centering
  \includegraphics{graphs/lobsters-timeline.pdf}
  \caption{Lobsters latency timeline at 1.5k pages per second across all pages
  with eviction. Figure shows the latency profile seen by the client over time,
  starting at the point when the first query is issued. Time increases along the
  x-axis, with each bin sampling twice as long as the previous one. The measured
  latency for each time bin is plotted on a logarithmic scale on the y-axis.
  Progressively lighter colors include more of the tail.}
  \label{f:lobsters-timeline}
\end{figure}

The cost of these misses is particularly visible when Noria starts with empty
state. This is equivalent to starting a more traditional caching system with an
empty (cold) cache, and having to ``warm'' it by filling in the most popular
entries. To measure this warming period, Figure~\ref{f:lobsters-timeline} shows
the latency profile seen by the Lobsters benchmark client over time, starting at
the point when the first query is issued. Time increases along the x-axis, and
the measured latency for each time bin is plotted on a logarithmic scale on the
y-axis. Progressively lighter colors indicate values further into the tail.

The figure shows that latency is initially high, but after a few seconds, the
mean and 95th percentile latency drop below 10 milliseconds. By the time a
minute has passed, the 99th percentile has followed suit. Since only a small
portion of the total computed state is cached (as shown in
Figure~\ref{f:lobsters-memory}), this supports the hypothesis that partial state
achieves low latency once the most commonly accessed results are cached.

\subsection{Impact on Tail Latency}

\begin{figure}[h]
  \centering
  \includegraphics{graphs/lobsters-memlimit-cdf.pdf}
  \caption{CDF of sojourn latency across all Lobsters pages at 1.5k pages per
  second with varying memory use. The figure depicts steady-state
  operation\,---\,the benchmark has been allowed to run for two minutes before
  the latency is measured.}
  \label{f:lobsters-mem-latency}
\end{figure}

Partial's trade-off is that of memory use versus tail latency; as you allocate
less memory to Noria, less of your tail can be pre-computed, and thus more of
your requests will be slow as it must be computed on-demand.
Figure~\ref{f:lobsters-mem-latency} shows this trade-off in the steady state%
\footnote{The benchmark runs for two minutes before latencies are sampled.}
of the application. It plots the CDF of the sojourn latency across all requests
with increasingly aggressive eviction policies. As Noria is asked to
reduce memory use by evicting more aggressively (darker purple lines), more
requests take longer, and the tail grows. In other words, the further you want
to reduce memory use, the more you pay in latency.

\begin{figure}[h]
  \centering
  \includegraphics{graphs/lobsters-durability-cdf.pdf}
  \caption{CDF of sojourn latency across all Lobsters pages at 1.5k pages per
  second with base tables in memory and on disk. The figure depicts steady-state
  operation\,---\,the benchmark has been allowed to run for two minutes before
  the latency is measured.}
  \label{f:lobsters-dur-latency}
\end{figure}

If base tables are not kept in memory, the cost of recomputing missing state
from the data in those base tables increases. Exactly how much depends on the
performance characteristics of the durability backend in use.
Figure~\ref{f:lobsters-dur-latency} shows a CDF of page latencies when Noria's
RocksDB backend is used, with writes going to a ramdisk. Latencies increase by
anywhere from 20-50\%, depending on the number of misses a given page request
experiences.

The reason why the whole curve shifts, rather than just the tail, is that these
CDFs are across \emph{all} the different page types in Lobsters. Each one issues
a different set of queries, and so their total time differs, as does the effect
of a longer tail. The exact shape of this curve, and how it shifts in response
to varying resources, depends on the application in question.

% \begin{inprogress}
% Full materialization is slightly faster than partial materialization because
% SOMETHING SOMETHING either join eviction or faster lookups due to no tombstones?
% \end{inprogress}

\subsection{Memory Use and Throughput}

Memory use can only be reduced so far before the system no longer keeps up with
the offered load. If some of the most frequently accessed (``hot'') query
results are not cached, the system will constantly have to re-compute those
results to satisfy reads that come in shortly after that query result is
evicted. This cache churn increases latency and decreases throughput, often
significantly. Essentially, the system will never finish warming the cache, and
latency will remain at the high levels shown early in
Figure~\ref{f:lobsters-timeline}. For Lobsters, this happens around the 18GB
mark. If the eviction is tuned to be more aggressive than that, Noria can no
longer sustain 1.5k pages per second.

Generally speaking, as throughput increases, so must the memory budget. The
intuition behind this is that the memory budget effectively dictates your hit
rate. The more requests are issued per second, the more misses (in absolute
terms) result from a given hit rate. If those misses in the tail are
distinct, Noria must satisfy \textbf{more} upqueries as load increases, while
also handling that added load.

\begin{listing}[h]
  \begin{minted}{sql}
    SELECT articles.*, COUNT(votes.user_id)
    FROM articles
    LEFT JOIN votes ON (articles.id = votes.article_id)
    GROUP BY articles.id
    WHERE articles.id = ?
  \end{minted}
  \caption{Simplified query for vote counting in Lobsters.}
  \label{l:votes}
\end{listing}

While this correlation between throughput and memory use exists in Lobsters, it
is difficult to show clearly as each page issues many different queries, and
overall load is relatively low. For this reason, the next set of benchmarks use
a simplified version of one particular query from Lobsters shown in
Listing~\ref{l:votes}. It counts the number of votes for an article, and
presents that alongside the article information. The benchmark issues requests
distributed as 99\% reads and 1\% writes (inserts into \texttt{votes}). The
access pattern is skewed such that 90\% of requests access 1\% of keys across
10M articles%
\footnote{The benchmark samples keys from a Zipfian distribution with a skew
factor ($\alpha$) of 1.15}. Load is generated by four clients, and each one
batches requests for a maximum of 10ms to reduce serialization overheads.

\begin{figure}[h]
  \centering
  \includegraphics{graphs/vote-throughput-memlimit.pdf}
  \caption{Achieved throughput vs 95th percentile request latency in vote with
  increasingly aggressive eviction. Offered load increases along the points on
  each line. A near-vertical line indicates that the system no longer keeps up
  with offered load.}
  \label{f:vote-throughput-memlimit}
\end{figure}

Figure~\ref{f:vote-throughput-memlimit} demonstrates the connection between
throughput and memory use. It shows throughput-latency lines for the vote
benchmark with progressively more aggressive eviction. Each point along each
line is a higher offered load; its x-coordinate is the achieved throughput, and
its y-coordinate is the measured median latency. When Noria no longer keeps up,
you see a ``hockey stick'' effect, where achieved throughput no longer
increases, while latency spikes.

The figure shows that as the offered load increases, Noria needs to use more
memory to keep up. If we do not bound memory use, Noria can keep up with higher
load at the cost of caching most of the dataset (about two thirds).

\section{Absolute Cache Performance}
\label{s:eval:kvperf}

Despite how error-prone the approach is, ad-hoc, per-application caching is
still common in practice. And while Noria eliminates most of the developer
burden of getting caching right, it must offer competitive performance with
manually constructed caching solutions to present a viable alternative.

Unfortunately, this is difficult to evaluate, since high-performance solutions
are often developed specifically for a given application, and not available as
general-purpose tools. And effectively applying the general-purpose tools that
\textbf{are} available, like memcache and Redis, requires significant effort on
the part of the application authors (or the evaluators). To manually add caching
support to Lobsters' ~80 queries, including thundering herd mitigation and
incremental updates, would be a massive undertaking.

This fact alone is, in essence, an argument for the Noria approach. The manual
effort involved in making Lobsters use Noria is minimal\,---\,just switch the
code to query Noria instead of MySQL, and you get automatic caching. In many
cases the application code can even be simplified, such as by removing manual
materialization decisions like the story ``hotness'' column described in
\S\ref{s:eval:alts}.

Nevertheless, an experiment to evaluate Noria's absolute performance compared to
a ``real'' cache is necessary. Without such a comparison, Noria can only claim
to be ``faster than MySQL'', but not ``as fast as a cache''.

The next experiment runs the vote benchmark from Listing~\ref{l:votes} against
Redis~\cite{redis}, a popular high-performance key-value store that is commonly
used as a caching backend. In an attempt to approximate how a carefully planned
and optimized application caching deployment might perform, it makes the
following modifications to the benchmark:

\begin{itemize}
 \item Every access hits in cache, to emulate perfect thundering herd mitigation
   and invalidation-avoidance schemes.
 \item Nearly all accesses (99.99\%) are reads, since writes would be
   bottlenecked by the backing store.
 \item Data is not stored anywhere except Redis.
 \item Accesses are batched to reduce serialization cost and increase
   throughput. Specifically, reads are \texttt{MGET}s, and writes are pipelined
    \texttt{INCRBY}s.
\end{itemize}

This is not a realistic use of Redis as a cache, and ignores the complexities of
integrating the cache with the application. It also assumes that cached query
results are never spread across more than one key in the cache. But it does
allow ``assuming the best'' about the underlying caching strategy and system.

\begin{figure}[h]
  \centering
  \includegraphics{graphs/vote-redis.pdf}
  \caption{Achieved throughput vs 95th \%-ile request latency in cache-optimized
  vote. Offered load increases along the points on each line. The vertical
  line indicates $16\times$ the highest Redis throughput, since Redis is
  single-threaded.}
  \label{f:vote-redis}
\end{figure}

Figure~\ref{f:vote-redis} shows a throughput-latency plot that explores the
performance profiles of Redis and Noria under these experimental conditions%
\footnote{Noria runs with the same modified access patterns as outlined for
Redis.}.
% For comparison, it also includes a MySQL + Redis implementation that
% stores votes and articles in MySQL, and uses the na\"ive write-back cache
% strategy outlined above.
Redis is not multi-threaded, and can only use one of the server's 16 cores, so
the figure also includes the Redis performance extrapolated to 16 cores. This is
an over-estimate, since to achieve this performance in practice, the
application's already-perfect caching scheme would need to also shard perfectly.
Noria implements the necessary synchronization internally to take advantage of
all the cores.

The results show that Noria achieves $\approx$42\% of the theoretical 16-core
performance of Redis. Given the idealized nature of this experiment, the exact
absolute numbers should be taken with a pile of salt, but they do provide an
upper bound of sorts for Redis' performance. That Noria approaches this
performance is a good indicator that Noria's cache hit performance is comparable
to that of an ad-hoc caching implementation. And again, Noria does so while
providing rich SQL queries, and without requiring application-specific caching
logic.

\section{Bringing Up New Views}
\label{s:eval:mig}

When the application issues a query that Noria has never seen before, Noria must
instantiate the dataflow for that query, along with any materializations it
might need. Without partial state, the system must do all the work to compute
the full state for the new view, and any internal operator state it depends on,
up front and all at once. And during that time, Noria's dataflow must spend
cycles on computing that new state, slowing down the processing of other
concurrent writes. The new view also cannot serve any reads until all the state
is computed.

\begin{listing}[h]
  \begin{minted}{sql}
    CREATE VIEW scores AS
      -- same vote count (each vote is a 1 rating)
      SELECT votes.article_id, COUNT(votes.user_id) AS score
      FROM votes
      GROUP BY votes.article_id
    UNION
      -- compute total rating score
      SELECT ratings.article_id, SUM(ratings.rating) AS score
      FROM ratings
      GROUP BY ratings.article_id;

    -- new view aggregates across votes and ratings
    SELECT articles.*, SUM(scores.score)
    FROM articles
    LEFT JOIN scores ON (articles.id = scores.article_id)
    GROUP BY articles.id
    WHERE articles.id = ?;
  \end{minted}
  \caption{Updated query for ``rating'' counting in Lobsters.}
  \label{l:ratings}
\end{listing}

Partial state enables such migrations to be instantaneous in many cases\,---\,if
the new view can be made partial it is instantiated as empty, and immediately
made available. It is then filled on demand as the application submits reads.
To demonstrate the different behavior of full and partial materialization for
migrations, the next benchmark makes a modification to the ``vote'' benchmark
from Listing~\ref{l:votes}. It introduces a new table, \texttt{ratings}, which
has ``ratings'' on a scale from 0 to 1 for each article instead of just a binary
0 or 1. It also add a new view, shown in Listing~\ref{l:ratings}, which combines
the existing votes with the new ratings to compute a total article score%
\footnote{By writing the query this way, votes and ratings can co-exist.}.

The benchmark inserts votes and issues the original vote query for 90 seconds,
and then introduces the new table and query (denoted as time 0). From then on,
it issues both votes and ratings, and queries both views without blocking every
10 milliseconds.

\begin{figure}[h]
  \centering
  \includegraphics{graphs/vote-migration.pdf}
  \caption{Time to set up and access a new view. Access pattern is skewed (Zipf;
  $\alpha = 1.15$) across 10M articles. Benchmark runs for 90s prior to
  migration (solid red line). The dashed vertical line denotes the end of the
  migration without partial state.}
  \label{f:vote-migration}
\end{figure}

Figure~\ref{f:vote-migration} plots the cache hit rate for reads from the new
view over time, as well as the write throughput over the course of the
experiment. Without partial state, the new view is not accessible until its
construction finishes after $\approx$23 seconds. During that time, the
application write performance drops substantially, as Noria must compute the
content of the new view.

With partial state, the view is immediately accessible, though its cache hit
rate is initially low. However, since there are a few very popular keys, the hit
rate quickly climbs to over 90\%. Since only results for requested keys are
computed, write throughput is mostly unaffected by the migration%
\footnote{Overall write throughput is lower after the migration since Noria must
now maintain two views, not just one.}.

The figure also exposes another interesting effect of using partial
materialization: increased write throughput. Without partial state, every write
must be processed to completion, since all results are cached. With partial
state, writes for keys that have not been read can be discarded early, as there
is no state in memory that must be updated, which increases throughput.

\section{Skew}
\label{s:eval:patterns}

Partial state is mainly useful if accesses are skewed towards a particular
subset of queries and data. When this is the case, caching a small number of the
application's computed state speeds up a significant fraction of requests. If
this is not the case, the likelihood of missing in the cache is inversely
proportional to the size of the cache, and you would need to cache computations
over most of the data to maintain a decent cache hit rate.

This kind of significant skew is present in Lobsters, which is what allows it to
run smoothly even when only a small fraction of results are cached. Significant
skew shows up across a wide range of other real-world
datasets~\cite{power1,power2,network-skew,large-skew-analysis}, including many
social networks~\cite{network-skew2, community-skew}. In a large public Amazon
dataset~\cite{amazon-skew}, the 100,000 most popular book titles (less than 5\%)
account for roughly 50\% of all book sales, and 75\% of the sales are for the
top 500,000 titles~\cite{zhang2020permutation}.

In the vote benchmark, the decision of which articles to fetch and vote for is
artificially skewed using a Zipfian probability distribution~\cite{zipf}; a
probability model that commonly describes skewed frequency distributions in
natural and random datasets. Specifically, given some number of elements $N$ and
a skew parameter $\alpha$, the normalized frequency of the $k$th element in a
Zipf distribution is given by

\begin{displaymath}
  f\left(k;\alpha,N\right)={\frac {1/k^{\alpha}}{\sum \limits _{n=1}^{N}(1/n^{\alpha})}}
\end{displaymath}

Every time the vote benchmark performs a read or a write, it samples a value,
$k$, in such a way that the likelihood of choosing a given $k$ is given by
$f\left(k;\alpha,N\right)$. A higher value for $\alpha$ means that smaller $k$
values will be sampled more frequently than larger $k$ values, increasing the
skew.

It is difficult to estimate the degree of skew for a complex application ahead
of time. But, because many datasets exhibit skew following something akin to a
Zipfian distribution, an analysis of vote may still yield some helpful
heuristics for application developers.

In vote, after $S$ samples (given by throughput $\times$ period), the expected
number of keys hit is the sum of the probability that each element $k$ is
sampled at least once during the benchmark, given by

\begin{displaymath}
  F(\alpha,N)={\sum \limits _{k=1}^{N} \left(1 - \left(1 - f(k; \alpha, N)\right)^{S}\right)}
\end{displaymath}

\begin{figure}[h]
  \centering
  \includegraphics{graphs/vote-formula.pdf}
  \caption{Probabilistic model of the fraction of 10M keys that are accessed
  (``hot'') during each eviction period ($\approx$ 1s) as throughput increases.
  Each line shows a different amount of skew. Skew (X/Y) denotes that X\% of
  requests come from Y\% of keys. More keys pushes the curve down.}
  \label{f:vote-formula}
\end{figure}

Figure~\ref{f:vote-formula} plots $F(\alpha, N)/N$ after one eviction cycle
($\approx$ 1s) for different degrees of skew ($\alpha$) with $N=10\text{M}$ as
throughput varies. That value corresponds to the expected fraction of keys
accessed between any two eviction cycles, and effectively sets a lower bound on
the fraction of the query results that must be cached. It thus also dictates
minimum memory use. While Noria \emph{could} maintain a smaller fraction of the
query results, the application would likely need those keys again shortly after
evicting them, and Noria would continuously compute and then discard frequently
accessed query results.

Also indicated on the figure is the lowest ratio achieved between operator state
size%
\footnote{This is measured using internal estimates of operator state size, not
VmRSS, so as to only measure the fraction of keys cached.}
with and without partial state in the vote benchmark at 250k operations per
second. The measured value of 0.8\% gets close to the expected value of 0.46\%
computed by the formula for the 90/1 skew that the vote benchmark uses. If the
eviction is tuned to be even more aggressive, the benchmark can no longer keep
up. This suggests that Noria is indeed able to function at close to the
predicted cache ratio, and that the model bears some association to observed
behavior.

At very high offered load, Noria can rarely get quite as low as the graph
indicates. For example, at 1M operations per second, Noria must maintain 21\% of
keys even though the model predicts that 1.4\% should be sufficient. There are
multiple related reasons for this.

First, Noria currently implements only \emph{randomized} eviction, so frequently
accessed keys will occasionally be evicted. When they do, a large number of
requests must wait for its result to be recomputed. With a less naive eviction
scheme, such as LRU, such evictions can be avoided, and hot keys will never
miss.

Second, more upqueries must be serviced per second. Since upqueries are
performed by the data flow, which is single-threaded along any given path, the
upquery processing itself becomes a bottleneck. To maintain acceptable latency,
Noria is forced to keep many more keys in cache than the model predicts so that
not too many upqueries occur.

And third, Noria's eviction runs at a fixed interval of one second. As offered
load increases, so too does the number of keys read, and the number of keys
cached in that one second. The eviction logic thus has more keys it needs to
evict each time it runs. This in turn takes up more data flow cycles on the
write path that could otherwise be dedicated to serving upqueries.
