This thesis is built on the belief that view materialization is useful, but
prohibitively costly to use with current solutions. It presents partial state as
a solution to this problem, which allows retaining the benefits of view
materialization at a fraction of the cost. Section \ref{s:eval:why} evaluates
the validity of this assumption, and the efficacy of partial state as a
solution.

Partial state enables a trade-off between memory use (cache size) and tail
latency (miss rate). Since many applications operate with a latency budget, the
parameters of this trade-off is important. Section \ref{s:eval:cost} explores
the trade-off space.

With partial state, applications can introduce new views without materializing
that entire view all up-front. Instead, with partial state, the view is
gradually materialized on-demand. This enables fast migrations, but also means
that queries to new views may take a while to be satisfied. Section
\ref{s:eval:warmup} looks at the post-migration period in detail to shed light
on this on-demand process.

The efficacy of partial state as a solution to the view materialization memory
use problem depends on appliation access patterns being sufficiently skewed that
keeping only some results cached reduces latency for a significant fraction of
requests. While it is impossible to predict the skew for an arbitrary
application's data and queries, section \ref{s:eval:patterns} gives a simple
theoretical model to help with estimation.

The experiments in this chapter revolve primarily around the Lobsters
news aggregator application. This application, the nature of the generated
workloads, and the experimental setup is covered in section \ref{s:eval:setup}.

\section{Experimental Setup}
\label{s:eval:setup}

\section{Benefits and Costs of View Materialization}
\label{s:eval:why}

% The work in this thesis is built on the belief that materialization is useful,
% and that partial materialization is necessary to make it practical for an
% important class of applications. Without materialization, queries must either be
% manually cached, which is labor-intensive and error-prone, or be computed on
% demand, which is slow. And with full state materialization, all queries are
% fast, but the memory cost is prohibitive for any but the smallest applications.
% Whereas partial state allows you to trade off application tail latency for
% reduced memory use.

In this section, I provide experimental and qualitative evidence that this
belief is warranted through analysis of the following questions:

\begin{figure}[h]
  \centering
  \includegraphics{graphs/lobsters-memory.pdf}
  \caption{Effect of partial materialization on memory use and throughput. Noria
  with eviction retains the increased throughput from materialization, but
  substantially reduces memory use. The savings from partial alone stem from
  many items never being accessed. With full materialization, Noria runs out of
  memory at $\approx$3k pages/second (â€ ).}
  \label{f:lobsters-memory}
\end{figure}

The primary motivation behind reducing application memory use is to reduce cost.
Higher memory use requires more memory, and more memory costs more, both to
purchase and in ongoing power consumption. An application that uses less
memory can scale further on the same hardware, or, conversely, can run on
smaller hardware than would otherwise be needed.

The key question for partially stateful dataflow then is whether it truly and
meaningfully reduces application memory use. Or, more accurately, whether
applications can run efficiently when only some of its query results are cached.
Intuitively, one might expect this to be true for many user-driven applications:
some queries may only be executed for rarely used features, and unpopular posts
may be accessed only very infrequently, or perhaps not at all, after they were
initially posted.

To validate this, I used production statistics from the open-source Lobsters
news aggregator~\cite{lobsters,lobsters-data} to build a benchmarking harness
that approximates the page access patterns the real Lobsters website sees. The
load generator is ``partially open-loop'' to ensure that clients maintain the
measurement frequency even during periods of high latency, and also measure
queueing time~\cite{frank-open-loop,open-loop-cautionary-tale}. Since Lobsters
is a small site, relatively speaking, the harness also lets me to proportionally
scale up the access frequency to evaluate a larger deployment with the same
data and query access patterns.

I then wrote a Noria implementation for each Lobsters page endpoint that
queries the same data, and performs the same writes as the real Ruby-on-Rails
application.%
\footnote{I did not run the Ruby-on-Rails code directly to avoid measuring
bottlenecks in Ruby and Rails.}
Together, these allow me to measure the throughput, latency, and memory overhead
that Noria would provide if used as the backend for Lobsters as it scales up.

Figure~\ref{f:lobsters-memory} shows the memory use%
\footnote{Note that this counts the data set size, not the resident virtual
memory.}
of stateful operators with
and without partial materialization at a scale-up factor of 4000 times normal
Lobsters load. Note that the figure does not show the memory used by the base
tables, as it is the same for all three configurations. 4000 was chosen as it is
the highest scaling factor that can run in the 128GB of memory of the host
machine without partial materialization.

The figure also shows the lowest memory limit Noria can sustain without when
eviction is enabled. Here, I run Noria with a progressively more aggressive
eviction policy, and stop once the backend can no longer keep up with offered
load. With partial materialization and eviction, Noria uses approximately
20$\times$ less memory for operator state than full materialization does.

\section{What is the cost of using partial state?}
\label{s:eval:cost}

\begin{figure}[h]
  \centering
  \includegraphics{graphs/lobsters-memlimit-cdf.pdf}
  \caption{CDF of Lobsters page sojourn latency in steady-state at 3000 pages
  per second as the memory use budget shrinks. The CDF is across all different
  page requests.}
  \label{f:lobsters-mem-latency}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics{graphs/lobsters-pages-cdf.pdf}
  \caption{CDF of Lobsters page latency in steady-state at 3000 pages per second
  for different pages without eviction. This does \textbf{not} measure queueing
  time.}
  \label{f:lobsters-pages-latency}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics{graphs/vote-memlimit-cdf.pdf}
  \caption{CDF of vote read request latency in steady-state at 1.6M requests per
  second as the memory use budget shrinks.}
  \label{f:vote-mem-latency}
\end{figure}

\begin{figure}[h]
  \centering
  \includegraphics{graphs/vote-throughput-memlimit.pdf}
  \caption{Achieved throughput vs median request latency in vote with
  different memory budgets. Offered load increases along the points on each
  line. At higher offered load, a higher memory budget is needed to keep up, or
  too many misses must be handled.}
  \label{f:vote-throughput-memlimit}
\end{figure}

Partial state's main drawback compared to traditional materialized views is
that the results for an application's query may not be known. When this happens,
the system must upquery the missing state, which takes time and consumes
resources otherwise dedicated to writes. The fundamental trade-off here is that
of memory use versus tail latency. The more memory you provide the partial
system with, the more of your tail will be pre-computed, and the more of your
requests will be fast. Conversely, you can use less memory if you are willing
for more of your tail to be computed on demand.

Figure~\ref{f:lobsters-mem-latency} explores this trade-off in more detail in
the context of the Lobsters experiment from the previous section. You can see
that as the memory limit shrinks, the fraction of requests in the tail that must
be computed increases, and thus tail latency increases.

\begin{inprogress}
  \textbf{Why on earth is tail latency here 10 seconds?}
\end{inprogress}

\begin{inprogress}
  It's not clear why ``full'' does so well here.
  Maybe it's because partial must store tombstones?
\end{inprogress}

The figure also demonstrates how much costlier upqueries are than cached
reads. Where a read that hits in cache only has to wait for serialization, an
upquery must wait for the requested data to be fetched and computed before it
finishes. If this happens to multiple queries the application issues for a
particular page to be rendered, that latency accumulates, increasing the tail
latency.

The figure above includes data across all the different Lobsters pages the
client requests over the course of the benchmark.
Figure~\ref{f:lobsters-pages-latency} breaks this down by four fairly
different page request types: viewing a story (most common), viewing the front
page, voting for a comment, and submitting a new story (least common).

\begin{inprogress}
  When I get the tail under control, the horizontal distance here is
  interesting, since it indicates fewer/more queries are issued. The tail is
  also be interesting, since it indicates how affected the given page is to
  evictions.
\end{inprogress}

Even at a scale factor of 4000$\times$, Lobsters only serves about 3000 pages
per second. While each page executes multiple queries, the absolute number of
queries per second is fairly low, which allows the system to get away with much
of the tail computed on-demand. To shed more light on the extremes of this
trade-off, I next present results for a simplified version of one particular
query from Lobsters shown in Listing~\ref{l:votes}. It just counts the number of
votes for an article, and presents that alongside the article information.

\begin{listing}[h]
  \begin{minted}{sql}
    SELECT articles.*, COUNT(votes.user_id)
    FROM articles
    LEFT JOIN votes ON (articles.id = votes.article_id)
    GROUP BY articles.id
    WHERE articles.id = ?
  \end{minted}
  \caption{Simplified query for vote counting in Lobsters.}
  \label{l:votes}
\end{listing}

Figure~\ref{f:vote-mem-latency} shows a similar latency CDF as for Lobsters, but
for a benchmark stressing this particular query. The benchmark issues requests
distributed as 95\% reads and 5\% writes (inserts into \texttt{votes}). The
access pattern is skewed (Zipf; $\alpha = 1.15$) across 5M articles. The figure
demonstrates the trade-off clearly. As you lower the memory limit, more requests
must be computed, and tail latency suffers.

\begin{inprogress}
  Writes are \textit{consistently} \textbf{much} slower with partial than
  without. That seems like a problem that I should look into.
\end{inprogress}

Another important aspect of the partial trade-off is that as throughput
increases, so must the memory budget. The intuition behind this is that the
memory budget effectively dictates your hit rate. The more requests are issued
per second, the more misses (in absolute terms) result from a given hit
fraction. If those misses in the tail are distinct, Noria must satisfy
\textbf{more} upqueries as load increases, while also handling that added load.

Figure~\ref{f:vote-throughput-memlimit} demonstrates this effect in practice. It
shows throughput-latency lines for the vote benchmark for different memory
budgets. Each point along each line is a higher offered load; its x-coordinate
is the achieved throughput, and its y-coordinate is the measured 90th percentile
latency. When the backend no longer keeps up, you see a ``boomerang'', where
achieved throughput no longer increases, while latency spikes. The figure shows
that as the offered load increases, Noria needs a larger memory budget to keep
up.

\section{Migrations}
\label{s:eval:cost:mig}

In the previous section, we looked at queries where partial lookups may miss due
to evictions, or due to the key not yet having been computed. But there's
another important use-case for partial that has a fairly different usage
pattern: migrations.

When the application issues a query that Noria has never seen before, Noria must
instantiate the dataflow for that query, along with any materializations it
might need. Partial state enables those migrations to be instantaneous in many
cases\,---\,if the new view can be made partial (ref section + later eval) it is
instantiated as empty, and immediately made available, and will be filled on
demand as the application submits reads.

In addition to memory use, the trade-off here also includes the migration time.
With full materialization, the system must to a potentially large amount of work
up-front to compute the full state for the new view, and any internal operator
state it depends on. And during that time, the related segments of the dataflow
must spend precious cycles on computing that new state, at the expense of other
concurrent writes. The new view also cannot serve any reads until all the state
is computed.

\begin{listing}[h]
  \begin{minted}{sql}
    CREATE VIEW scores AS
      -- same vote count (each vote is a 1 rating)
      SELECT votes.article_id, COUNT(votes.user_id) AS score
      FROM votes
      GROUP BY votes.article_id
    UNION
      -- compute total rating score
      SELECT ratings.article_id, SUM(ratings.rating) AS score
      FROM ratings
      GROUP BY ratings.article_id;

    -- new view aggregates across votes and ratings
    SELECT articles.*, SUM(scores.score)
    FROM articles
    LEFT JOIN scores ON (articles.id = scores.article_id)
    GROUP BY articles.id
    WHERE articles.id = ?;
  \end{minted}
  \caption{Updated query for ``rating'' counting in Lobsters.}
  \label{l:ratings}
\end{listing}

\begin{figure}[h]
  \centering
  \includegraphics{graphs/vote-migration.pdf}
  \caption{Time to set up and access a new view. Access pattern is skewed (Zipf;
  $\alpha = 1.15$) across 10M articles. Benchmark runs for 90s prior to
  migration (red line). Dashed vertical line denotes the end of the migration
  for full materialization.}
  \label{f:vote-migration}
\end{figure}

To measure this, this next benchmark makes a modification to the ``vote''
benchmark illustrated by Listing~\ref{l:votes}. It introduces a new table,
\texttt{ratings}, which has ``ratings'' on a scale from 0 to 1 for each article
instead of just a binary 0 or 1. It also add a new view, shown in
Listing~\ref{l:ratings}, which combines the existing votes with the new ratings
to compute a total article score%
\footnote{By writing the query this way, votes and ratings can co-exist.}.
The benchmarker inserts votes and issues the original vote query
(Listing~\ref{l:votes}) for 90 seconds, and then introduces the new table and
query (denoted as time 0). From then on, it issues both votes and ratings, and
queries both views without blocking every 10 milliseconds.

Figure~\ref{f:vote-migration} plots the cache hit rate for reads from the new
view over time. This figure demonstrates the upfront cost of full
materialization; when the new query is installed, Noria must expend significant
resources to fully compute the new view. During this time, it also slows down
concurrent writes, as it must use that part of the dataflow to compute the data
needed for the new view. The new view can only satisfy read requests once it has
been fully computed. In the interim, the application must wait.

With partial materialization enabled, Noria exhibits very different behavior:
the migration completes instantly, and there is no downtime for writes. The new
view is available for reads immediately, though initially many reads miss. Those
reads must wait for an upquery to finish, but the hit rate increases quickly due
to the data skew.

\begin{inprogress}
The figure also exposes another interesting effect of using partial
materialization: increased write throughput. With full materialization, every
write must be processed to completion, but with partial, writes for keys that
have not been read can be discarded early. This produces an increase in
throughput, with diminishing impact as more keys are read. With eviction, the
increased write throughput would persist as the number of keys would plateau.
Since the dataflow must now process both votes and ratings, each write type sees
about half the write throughput prior to the migration.
\end{inprogress}

\begin{inprogress}
  Why do we only reach ~90\% hit rate? That suggests a decent number of reads
  are still happening, but miss. I guess the read rate is pretty low, so that's
  probably why.
\end{inprogress}

\begin{inprogress}
  Should we show partial with eviction?
  Uniform distribution?
\end{inprogress}

Like other caching systems, Noria suffers when hot keys are not in memory. When
the application adds a new view, this will necessarily be the case, just as when
an application adds a new cache entry in any other cache system. As in
traditional systems, applications may want to ``warm'' the cache so that the
hottest keys are present before all client load is shifted to the new view. How
long that warming takes depends on the query in question, the size of the data
that must be computed, and the skew in the data.

Figure~\ref{f:vote-migration} shows that, for the vote query, the hit rate is
85\% after about 20s, but this is a very limited sample. To get a better idea of
how this time varies for a wider variety of situations, we return to the
Lobsters experiment.

Figure~\ref{f:lobsters-timeline} shows the latency profile seen by the Lobsters
benchmark client over time, starting at the point when the first query is
issued. Progressively brighter colors include more of the tail. The figure
shows that latency is initially high, but after just a few seconds, the mean and
90th percentile latency drop below 10 milliseconds. The 95th percentile follows
suit after 20 seconds. By the time a minute has passed, most pages are satisfied
in single-digit milliseconds, and 99th percentile latency is 20 milliseconds.

\begin{figure}[t]
  \centering
  \includegraphics{graphs/lobsters-timeline.pdf}
  \caption{Lobsters latency timeline at 1.5k pages per second across all pages
  without eviction. \textbf{Both axes use a logarithmic scale.} Brighter colors
  include more of the tail.}
  \label{f:lobsters-timeline}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics{graphs/lobsters-timeline-evict.pdf}
  \caption{Lobsters latency timeline at 1.5k pages per second across all pages
  with eviction. \textbf{Both axes use a logarithmic scale.} Brighter colors
  include more of the tail.}
  \label{f:lobsters-timeline-evict}
\end{figure}

\begin{figure}[t]
  \centering
  \includegraphics{graphs/vote-timeline.pdf}
  \caption{Vote read latency timeline at 800k operations per second without
  eviction. \textbf{Both axes use a logarithmic scale.} Brighter colors include
  more of the tail. Note that y axis is one order of magnitude lower than for
  the Lobsters plots.}
  \label{f:vote-timeline}
\end{figure}

\begin{inprogress}
  Tail latency increases towards the right due to log-time; as we go right in
  the plot, we're sampling for twice as long, and we see more of tail.
\end{inprogress}

\begin{inprogress}
  log-log plot...
\end{inprogress}

\section{How dependent is partial state on application access patterns?}
\label{s:eval:patterns}

\begin{figure}[h]
  \centering
  \includegraphics{graphs/vote-formula.pdf}
  \caption{Probabilistic model of the fraction of 5M keys that are accessed
  (``hot'') during each eviction period ($\approx$ 2s) as throughput increases.
  Each line shows a different amount of skew. Skew (X/Y) denotes that X\% of
  requests come from Y\% of keys. More keys pushes the curve down. With good
  eviction, memory use will follow this curve.}
  \label{f:vote-formula}
\end{figure}

Partial state is mainly useful if accesses are skewed towards particular queries
and particular data. When this is the case, caching a small number of the
application's computed state speeds up a significant fraction of requests. If
this is not the case, the likelihood of missing in the cache is inversely
proportional to the size of the cache, and you would need to cache computations
over most of the data to maintain a decent cache hit rate.

The results in Figure~\ref{f:lobsters-mem-latency} suggest that skew is indeed
present in certain applications\,---\,Lobsters can run efficiently even with a
constrained memory budget. Other papers on the subject of data access patterns
and caching strategies also suggest the same thing \textbf{MISSING CITES}
(Twitter/Facebook/log-normal/zipf/skew).

The degree of skew dictates how aggressively you can set the memory budget.
For example, for the Zipf distribution used in the vote benchmark, it is
possible to probabilistically compute the access pattern the benchmark will
experience as the throughput varies. Given some number of elements $N$ and a
skew parameter $\alpha$, the normalized frequency of the $k$th element in a Zipf
distribution is given by

\begin{displaymath}
  f(k;\alpha,N)={\frac {1/k^{\alpha}}{\sum \limits _{n=1}^{N}(1/n^{\alpha})}}
\end{displaymath}

The expected number of keys hit after a certain number of samples, $S$ (given by
throughput $\times$ period), is then the sum of the probability that each
element $k$ is sampled at least once during the benchmark, given by

\begin{displaymath}
  F(\alpha,N)={\sum \limits _{k=1}^{N} \left(1 - \left(1 - f(k; \alpha, N)\right)^{S}\right)}
\end{displaymath}

Figure~\ref{f:vote-formula} plots $F(\alpha, N)/N$ after one eviction cycle
($\approx$ 2s) worth of samples for different degrees of skew ($\alpha$) with
$N=5\text{M}$ as throughput varies. The result demonstrates how key the
throughput is to determining the memory budget. As throughput increases, more
keys are accessed during any given time period, and need to be cached to not
impact the tail latency too severely. It also shows that eviction is critical;
at high offered load, even very skewed distributions quickly hit much of the
tail, which should therefore be cached, and memory use balloons as a result.

\section{How does partial state compare to existing solutions?}
\label{s:eval:existing}

\begin{figure}[h]
  \centering
  \includegraphics{graphs/vote-redis.pdf}
  \caption{Achieved throughput vs 90th \%-ile request latency in vote with Redis
  and Noria. Offered load increases along the points on each line.}
  \label{f:vote-redis}
\end{figure}

This is a difficult question to answer. High-performance solutions are
often developed specifically for a given application, and not available
as general-purpose tools (ref facebook memcache, need cites here).
And applying the general-purpose tools that \textbf{are} available (memcache,
redis) effectively requires significant effort on the part of the application
authors (or the evaluators). To manually add caching support to Lobsters' ~80
queries, including thundering herd mitigation and incremental updates, would be
a massive undertaking.

In some sense, this alone is an argument for Noria's approach. Since the
database uses information that is already present (in the form of application
queries) to automatically optimize accesses through materialization, it is
relatively easy to take advantage of the benefits that Noria provides.

Nonetheless, to shed some light on the absolute performance that Noria provides
against a caching system, I include below a performance comparison between Noria
and Redis. To approximate how a carefully planned and optimized application
caching deployment might perform, it runs a workload that is idealized for a
caching system:

\begin{itemize}
 \item Every access hits in cache, to emulate perfect thundering herd mitigation
   and invalidation-avoidance schemes.
 \item Nearly all accesses (99.9\%) are reads, since writes would be
   bottlenecked by the backing store.
 \item All accesses are for a single integer value, to emulate a system that has
   perfect cache coverage. Request access keys are chosen according to a Zipfian
    distribution with $\alpha = 1.15$, so that 1\% of keys make up 90\% of
    requests.
 \item Accesses are batched to reduce serialization cost and increase
   throughput. Specifically, reads are \texttt{MGET}s, and writes are pipelined
    \texttt{INCRBY}s.
\end{itemize}

This is not a realistic use of Redis, but it allows us to ``assume the best''
about the underlying caching strategy and system. The benchmark runs for four
minutes, and then samples latencies for another two minutes.

Figure~\ref{f:vote-redis} shows a throughput-latency that explores the
performance profiles of Redis and Noria under these experimental conditions.
Noria scales much further, primarily because Redis is single-threaded, which
necessarily limits its performance. Noria stops scaling when the single core
that processes writes for the dataflow path for the vote query reaches capacity.

If we assume perfect sharding of both reads and writes, Redis should be able to
support 16 times the load on the 16-core host machine, as shown by the dashed
orange line at ~85Mops/s. Noria is about an order of magnitude short of that
mark. However, beyond the simplifications above, Noria also does significantly
more work than Redis: it allows rich SQL queries, transfers full rows, not just
a single integer, and does not require sharding. Noria has also not seem the
same amount of optimization as Redis has gone through over the years. Taken
together, this suggests that Noria's performance, in absolute terms, is
competitive with manual caching schemes.
