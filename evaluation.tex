This thesis is built on the belief that view materialization is useful, but
prohibitively costly to use with current solutions. It presents partial state as
a solution to this problem, which allows retaining the benefits of view
materialization at a fraction of the cost. Section \ref{s:eval:why} evaluates
the validity of this assumption, and the efficacy of partial state as a
solution.

With partial state, only a subset of each view is materialized, and missing
results are computed on-demand. This reduces memory use, but also means that
some queries take a while to be satisfied. Ultimately, partial state presents a
trade-off between memory use (cache size) and tail latency (miss rate). Section
\ref{s:eval:cost} explores the implications of this trade-off.

Partial state allows applications to introduce new views without materializing
that entire view all up-front. Instead, the view is gradually materialized
on-demand. This enables fast adoption of new queries, but also means that
queries to new views are initially slow. Section \ref{s:eval:mig} evaluates such
partial state migrations.

The efficacy of partial state as a solution to the view materialization memory
use problem depends on appliation access patterns being sufficiently skewed that
keeping only some results cached reduces latency for a significant fraction of
requests. While it is impossible to predict the skew for an arbitrary
application's data and queries, section \ref{s:eval:patterns} gives a simple
theoretical model to help with estimation.

The experiments in this chapter revolve primarily around the Lobsters
news aggregator application. This application, the nature of the generated
workloads, and the experimental setup is covered in section \ref{s:eval:setup}.

\section{Experimental Setup}
\label{s:eval:setup}

The evaluation section revolves primarily around the Lobsters news aggregator
web application at \url{https://lobste.rs}~\cite{lobsters}. This application was
chosen because it is open-source (so we can see what queries it issues), because
it resembles many larger-scale applications (like Hacker News or Reddit), and
because a number of statistics are available about the site's data popularity
distributions and access patterns~\cite{lobsters-data}. Lobsters does not
implement caching, though its schema and queries do maintain columns of derived
values.

The thesis evaluation uses a workload generator that issues page requests
according to the available statistics~\cite{generator}. The generator supports
scaling up the rate of access to emulate a larger user base for benchmarking.
Since Lobsters is a Ruby-on-Rails application, which introduces significant
overheads, all experiments use an adapter that turns page requests directly into
the queries the real Lobsters code would issue for that same page request.

Experiments run on Amazon EC2 m5n.4xlarge instances, which have 16 vCPUs and
128GB of memory. The server is always given a dedicated host, while load
generating clients are split across one or more instances depending on the
desired load factor. Experiments are run for $6\sfrac{1}{2}$ minutes unless
otherwise specified.

The benchmark setups are all ``partially open-loop'': clients generate load
according to a workload-dictated distribution of interarrival-times, and has a
limited number of backend requests outstanding, queueing additional requests.
This ensures that clients maintain the measurement frequency even during periods
of high latency~\cite{frank-open-loop}. The test harness measures offered
request throughput and ``sojourn time''~\cite{open-loop-cautionary-tale}, which
is the delay from request generation until a response returns from the backend.

Memory use is measured as the sum of the size of each row in each view. Memory
overhead introduced by data structures and other program logic is therefore not
included in measurements.

% We seed the database with 9.2k users, 40k stories and 120k comments---the
% size of the real Lobsters deployment---and run increasing request loads to
% push the different setups to their limits.

\section{Benefits and Costs of View Materialization}
\label{s:eval:why}

The core argument of this thesis is that partial state make view materialization
feasible. Bundled up in that argument are several intertwined questions that
must be answered before further evaluation of partial state is interesting:

\begin{enumerate}
    \item Why is view materialization desirable?
    \item Why is view materialization not feasible currently?
    \item Does partial state improve on this situation?
\end{enumerate}

\begin{figure}[h]
  \centering
  \includegraphics{graphs/lobsters-memory.pdf}
  \caption{Effect of partial materialization on memory use and throughput. Noria
  with eviction retains the increased throughput from materialization, but
  substantially reduces memory use. The savings from partial alone stem from
  many pages never being visited. With full materialization, Noria runs out of
  memory at $\approx$3k pages/second (â€ ).}
  \label{f:lobsters-memory}
\end{figure}

Figure~\ref{f:lobsters-memory} attempts to give insight into all of these
questions. In this experiment, the Lobsters workload generator provides
progressively higher offered load in different configurations until each target
configuration can no longer keep up\footnote{When median latency exceeds 50ms.}.
It then measures memory use beyond what is required for base table storage at
that load factor.

The figure shows that view materialization (as provided by Noria) improves
performance by almost $6\times$. However, it also shows that without
partial state, view materialization requires significant amounts of memory, even
for the relatively small Lobsters dataset. So much so that, with full
materialization, Noria runs out of memory on the server host before saturating
all cores.

Even if the working set of the application \emph{can} fit in memory on a
sufficiently large machine, reducing application memory use also reduces cost.
Higher memory use requires more memory, and more memory costs more, both to
purchase and in ongoing power consumption. An application that uses less memory
can scale further on the same hardware, or, conversely, can run on smaller
hardware than would otherwise be needed.

The memory use drops by almost $5\times$ with partial state. This is a direct
result of the skew in Lobsters data popularity and access patterns. Many pages
are simply never visited over the course of the benchmark, and so need not be
brought into the cache. By reducing the memory use so significantly, Noria with
partial state is also able to support more load than with full materialization.

Partial state enables eviction of infrequently accessed results, which in turn
reduces memory use by an additional $7\times$, for a $32\times$ overall
reduction in memory use. With eviction, Noria only has to maintain the most
frequently accessed results, and can stop propagating writes for cold results
more aggressively, which further improves the sustainable load. Taken together,
Noria's partial state enables running lobsters at $9.7\times$ the load, using
only $11\%$ more memory than with MySQL\footnote{At 3.8k pages/s, the base table
size at the end of the benchmark is $\approx2.3$GB.}.

\begin{inprogress}
  Server not nearly fully loaded for any of the Noria results. Bottleneck is one
  particular write path, which is single-threaded.
\end{inprogress}

% The key question for partially stateful dataflow then is whether it truly and
% meaningfully reduces application memory use. Or, more accurately, whether
% applications can run efficiently when only some of its query results are cached.
% Intuitively, one might expect this to be true for many user-driven applications:
% some queries may only be executed for rarely used features, and unpopular posts
% may be accessed only very infrequently, or perhaps not at all, after they were
% initially posted.

\section{The Memory/Latency Trade-off}
\label{s:eval:cost}

Partial state's main drawback compared to traditional materialized views is
that the results for an application's query may not be known. Or, stated
differently, some reads may miss. When this happens, the system must upquery the
missing state, which takes time and consumes resources otherwise dedicated to
writes. This shows up as increased tail latency for the application; queries
whose results are not known must wait to be computed.

\begin{figure}[t]
  \centering
  \includegraphics{graphs/lobsters-timeline.pdf}
  \caption{Lobsters latency timeline at 1.5k pages per second across all pages
  with eviction. \textbf{Both axes use a logarithmic scale.} Lighter colors
  include more of the tail.}
  \label{f:lobsters-timeline}
\end{figure}

The cost of these misses is particularly visible when Noria starts with empty
state. This is equivalent to starting a more traditional caching system with an
empty (cold) cache, and having to ``warm'' it by filling in the most popular
entries. Figure~\ref{f:lobsters-timeline} shows the latency profile seen by the
Lobsters benchmark client over time, starting at the point when the first query
is issued. Progressively lighter colors include more of the tail. The figure
shows that latency is initially high, but after $\approx10$ seconds, the mean
and 95th percentile latency drop below 10 milliseconds. By the time a minute has
passed, the 99th percentile has followed suit.

\begin{inprogress}
  I believe much of the remaining tail latency is due to some remaining
  non-amortized resizes. Just visually, it seems like they might be worth
  fixing. It \emph{could} also be due to log-time; as we go right in the plot,
  we're sampling for twice as long, and we see more of tail.
\end{inprogress}

This particular curve is likely to be a common one\,---\,initially, all the
commonly-accessed query results are not in cache, so requests are slow. But
as those results are filled in, latency quickly drops such that only the
infrequently accessed query results must be computed on-demand.

\begin{figure}[h]
  \centering
  \includegraphics{graphs/lobsters-memlimit-cdf.pdf}
  \caption{CDF of Lobsters page sojourn latency at 3000 pages per second with
  different amounts of allocated memory. The CDF is across all different page
  requests. The figure depicts steady-state operation---the benchmark has been
  allowed to run for two minutes before the latency is measured.}
  \label{f:lobsters-mem-latency}
\end{figure}

Ultimately, the trade-off here is that of memory use versus tail latency; as you
allocate less memory to Noria, less of your tail can be pre-computed, and thus
more of your requests will be slow as it must be computed on-demand.
Figure~\ref{f:lobsters-mem-latency} shows this trade-off more clearly;
evicting more aggressively to reduce memory use makes more requests take longer,
and increases the tail latency. The further you want to reduce memory use, the
more you pay in latency. The exact shape of this curve, and how it shifts in
response to varying resources, depends on the application in question.
\S\ref{s:eval:patterns} attempts to give a useful approximation heuristic for
other applications.

\begin{inprogress}
Full materialization is slightly faster than partial materialization because
SOMETHING SOMETHING either join eviction or faster lookups due to no tombstones?
\end{inprogress}

\begin{inprogress}
The reason why the whole curve shifts, rather than just the tail, is that this
CDF is across \emph{all} the different page types in Lobsters. Each one issues a
different set of queries, and so their total time differs, as does the effect of
a longer tail. Note sure if it's useful to address this in the text?
\end{inprogress}

% \begin{figure}[h]
%   \centering
%   \includegraphics{graphs/lobsters-pages-cdf.pdf}
%   \caption{CDF of Lobsters page latency in steady-state at 3000 pages per second
%   for different pages without eviction. This does \textbf{not} measure queueing
%   time.}
%   \label{f:lobsters-pages-latency}
% \end{figure}
%
% The figure above includes data across all the different Lobsters pages the
% client requests over the course of the benchmark.
% Figure~\ref{f:lobsters-pages-latency} breaks this down by four fairly
% different page request types: viewing a story (most common), viewing the front
% page, voting for a comment, and submitting a new story (least common).

The memory use can only be reduced so far before the system can no longer keep
up with the offered load. If some of the most frequently accessed query results
are not cached, the system will spend all of its time re-computing those
results, latency will spike, and throughput will drop. For Lobsters, this
happens around the 150MB mark. If the eviction is tuned to evict below that
threshold, it can no longer sustain 3000 pages per second.

Generally speaking, as throughput increases, so must the memory budget. The
intuition behind this is that the memory budget effectively dictates your hit
rate. The more requests are issued per second, the more misses (in absolute
terms) result from a given hit fraction. If those misses in the tail are
distinct, Noria must satisfy \textbf{more} upqueries as load increases, while
also handling that added load.

\begin{listing}[h]
  \begin{minted}{sql}
    SELECT articles.*, COUNT(votes.user_id)
    FROM articles
    LEFT JOIN votes ON (articles.id = votes.article_id)
    GROUP BY articles.id
    WHERE articles.id = ?
  \end{minted}
  \caption{Simplified query for vote counting in Lobsters.}
  \label{l:votes}
\end{listing}

To demonstrate this effect at higher load, the next set of benchmarks use a
simplified version of one particular query from Lobsters shown in
Listing~\ref{l:votes}. It counts the number of votes for an article, and
presents that alongside the article information. The benchmark issues requests
distributed as 95\% reads and 5\% writes (inserts into \texttt{votes}). The
access pattern is skewed such that 90\% of requests access 1\% of keys (Zipf;
$\alpha = 1.15$) across 5M articles.

\begin{figure}[h]
  \centering
  \includegraphics{graphs/vote-throughput-memlimit.pdf}
  \caption{Achieved throughput vs median request latency in vote with
  different memory budgets. Offered load increases along the points on each
  line. At higher offered load, a higher memory budget is needed to keep up, or
  too many misses must be handled.}
  \label{f:vote-throughput-memlimit}
\end{figure}

Figure~\ref{f:vote-throughput-memlimit} demonstrates the connection between
throughput and memory use. It shows throughput-latency lines for the vote
benchmark for different memory allowances. Each point along each line is a
higher offered load; its x-coordinate is the achieved throughput, and its
y-coordinate is the measured median latency. When the backend no longer keeps
up, you see a ``hockey stick'' effect, where achieved throughput no longer
increases, while latency spikes. The figure shows that as the offered load
increases, Noria needs progressively more memory to keep up.

\section{Bringing Up New Views}
\label{s:eval:mig}

When the application issues a query that Noria has never seen before, Noria must
instantiate the dataflow for that query, along with any materializations it
might need. Partial state enables those migrations to be instantaneous in many
cases\,---\,if the new view can be made partial (ref section + later eval) it is
instantiated as empty, and immediately made available, and will be filled on
demand as the application submits reads.

In addition to memory use, the trade-off here also includes the migration time.
With full materialization, the system must to a potentially large amount of work
up-front to compute the full state for the new view, and any internal operator
state it depends on. And during that time, the related segments of the dataflow
must spend precious cycles on computing that new state, at the expense of other
concurrent writes. The new view also cannot serve any reads until all the state
is computed.

\begin{listing}[h]
  \begin{minted}{sql}
    CREATE VIEW scores AS
      -- same vote count (each vote is a 1 rating)
      SELECT votes.article_id, COUNT(votes.user_id) AS score
      FROM votes
      GROUP BY votes.article_id
    UNION
      -- compute total rating score
      SELECT ratings.article_id, SUM(ratings.rating) AS score
      FROM ratings
      GROUP BY ratings.article_id;

    -- new view aggregates across votes and ratings
    SELECT articles.*, SUM(scores.score)
    FROM articles
    LEFT JOIN scores ON (articles.id = scores.article_id)
    GROUP BY articles.id
    WHERE articles.id = ?;
  \end{minted}
  \caption{Updated query for ``rating'' counting in Lobsters.}
  \label{l:ratings}
\end{listing}

To measure this, this next benchmark makes a modification to the ``vote''
benchmark illustrated by Listing~\ref{l:votes}. It introduces a new table,
\texttt{ratings}, which has ``ratings'' on a scale from 0 to 1 for each article
instead of just a binary 0 or 1. It also add a new view, shown in
Listing~\ref{l:ratings}, which combines the existing votes with the new ratings
to compute a total article score%
\footnote{By writing the query this way, votes and ratings can co-exist.}.
The benchmark inserts votes and issues the original vote query
(Listing~\ref{l:votes}) for 90 seconds, and then introduces the new table and
query (denoted as time 0). From then on, it issues both votes and ratings, and
queries both views without blocking every 10 milliseconds.

\begin{figure}[h]
  \centering
  \includegraphics{graphs/vote-migration.pdf}
  \caption{Time to set up and access a new view. Access pattern is skewed (Zipf;
  $\alpha = 1.15$) across 10M articles. Benchmark runs for 90s prior to
  migration (solid red line). The dashed vertical line denotes the end of the
  migration for full materialization.}
  \label{f:vote-migration}
\end{figure}

Figure~\ref{f:vote-migration} plots the cache hit rate for reads from the new
view over time, as well as the write throughput over the course of the
experiment. The hit rate plot demonstrates the upfront cost of full materialization;
when the new query is installed, Noria must expend significant
resources to fully compute the new view. The write throughput plot shows that
during this time, it also slows down concurrent writes, as it must use that part
of the dataflow to compute the data needed for the new view. The new view can
only satisfy read requests once it has been fully computed. In the interim, the
application must wait.

With partial state enabled, Noria exhibits very different behavior: the
migration completes instantly, and there is no downtime for writes. The new view
is available for reads immediately, though initially many reads miss, and would
thus take longer to complete as they must wait for an upquery to finish. Due to
the data skew, the hit rate increases quickly, and after only a few seconds the
vast majority of requests hit in cache.

The figure also exposes another interesting effect of using partial
materialization: increased write throughput. With full materialization, every
write must be processed to completion, but with partial state, writes for keys
that have not been read can be discarded early, which increases throughput.

% Figure~\ref{f:vote-migration} shows that, for the vote query, the hit rate is
% 85\% after about 20s, but this is a very limited sample. To get a better idea of
% how this time varies for a wider variety of situations, we return to the
% Lobsters experiment.
% 
% \begin{figure}[t]
%   \centering
%   \includegraphics{graphs/vote-timeline.pdf}
%   \caption{Vote read latency timeline at 800k operations per second without
%   eviction. \textbf{Both axes use a logarithmic scale.} Brighter colors include
%   more of the tail. Note that y axis is one order of magnitude lower than for
%   the Lobsters plots.}
%   \label{f:vote-timeline}
% \end{figure}

\section{Skew}
\label{s:eval:patterns}

Partial state is mainly useful if accesses are skewed towards a particular
subset of queries and data. When this is the case, caching a small number of the
application's computed state speeds up a significant fraction of requests. If
this is not the case, the likelihood of missing in the cache is inversely
proportional to the size of the cache, and you would need to cache computations
over most of the data to maintain a decent cache hit rate.

Significant skew shows up across a wide range of real-world
datasets~\cite{power1,power2,network-skew}, including many social
networks~\cite{network-skew2, community-skew}. In a large public Amazon
dataset~\cite{amazon-skew}, the 100,000 most popular book titles (less than 5\%)
account for roughly 50\% of all book sales, and 75\% of the sales are for the
top 500,000 titles~\cite{zhang2020permutation}. In the Millennium
simulation~\cite{large-skew}, an important astrophysical data set that contains
more than 18 million trees, mass distribution among the trees is highly skewed,
with the 7 most frequent values appearing over 20 million times each, while
almost 75\% of the values appear no more than 10
times~\cite{large-skew-analysis}. This is also the case for the Lobsters
application, which is what allows it to run smoothly even when only a small
fraction of results are cached.

The degree of skew dictates how aggressively you can reduce the amount of memory
you make available for partial state. But it can be difficult to estimate this
ahead of time for a complex application. The Zipf distribution used in the vote
benchmark provides what may be a useful heuristic, since it allows
probabilistically computing the fraction of the data set that is accessed over a
period of time. Given some number of elements $N$ and a skew parameter $\alpha$,
the normalized frequency of the $k$th element in a Zipf distribution is given by

\begin{displaymath}
  f(k;\alpha,N)={\frac {1/k^{\alpha}}{\sum \limits _{n=1}^{N}(1/n^{\alpha})}}
\end{displaymath}

The expected number of keys hit after a certain number of samples, $S$ (given by
throughput $\times$ period), is then the sum of the probability that each
element $k$ is sampled at least once during the benchmark, given by

\begin{displaymath}
  F(\alpha,N)={\sum \limits _{k=1}^{N} \left(1 - \left(1 - f(k; \alpha, N)\right)^{S}\right)}
\end{displaymath}

\begin{figure}[h]
  \centering
  \includegraphics{graphs/vote-formula.pdf}
  \caption{Probabilistic model of the fraction of 5M keys that are accessed
  (``hot'') during each eviction period ($\approx$ 2s) as throughput increases.
  Each line shows a different amount of skew. Skew (X/Y) denotes that X\% of
  requests come from Y\% of keys. More keys pushes the curve down. With good
  eviction, memory use will follow this curve.}
  \label{f:vote-formula}
\end{figure}

Figure~\ref{f:vote-formula} plots $F(\alpha, N)/N$ after one eviction cycle
($\approx$ 2s) for different degrees of skew ($\alpha$) with $N=5\text{M}$ as
throughput varies. This corresponds to the expected fraction of keys accessed
between any two eviction cycles.This value effectively sets a lower bound on
the fraction of the query results that must be cached, and thus also dictates
minimum memory use. While you \emph{could} maintain a smaller fraction of the
query results, you would likely need those keys again shortly after evicting
them, and Noria would continuously compute and then discard frequently accessed
query results.

In practice, Noria can rarely get quite as low as the graph indicates. Eviction
is performed on the write path, so if the workload is write-bound, the system
may not keep up if eviction happens as aggressively as is needed to reach these
bounds. For example, at 800k requests/s with 90/1 skew, the model predicts that
only $\approx3.5$\% of keys need be materialized, but Noria can only get to
$\approx28$\% of the fully materialized cache size before write latency
increases dramatically.

\begin{inprogress}
  We have a couple of options here:
    1) try to find ways to optimize eviction;
    2) lengthen the eviction interval;
    3) run with lower skew at lower throughput; or
    4) run with a lower write rate (currently 95/5).
  None of these are ideal, but 3.5\% vs 28\% is also not particularly
  compelling. We could run at lower load, but then the formula number also
  lowers comparatively.
\end{inprogress}

\section{Rolling Your Own}
\label{s:eval:existing}

Many applications already require lower latency and higher throughput than
straightforward SQL queries against traditional relational databases can
provide. Developers often jump through hoops to squeeze out the performance they
need, usually through a mix of manual query optimization and intricate caching
logic. For example, in Lobsters, it would be prohibitively expensive to compute
the ``hotness'' of a story directly in the queries, especially in the context of
computing the front page view. Instead, the Lobsters developers add a computed
column, \texttt{hotness}, to the \texttt{story} table. This form of manual
materialization is common in web applications. It is necessary to make reads
fast, but also requires that the developers remember to update the computed
column whenever relevant data changes. In Lobsters, this means whenever:

\begin{itemize}
    \item a story is upvoted or downvoted.
    \item a comment is added to a story.
    \item a comment on a story is upvoted or downvoted.
    \item a comment or vote is deleted.
    \item one story is merged into another.
\end{itemize}

There are several such computed columns in Lobsters, and all write paths must
ensure that they correctly update all related computed values. This process is
manual and error-prone, and is entirely unnecessary with Noria.

Lobsters does not include a separate caching layer, which would also require
similar logic to update or invalidate results in the cache as appropriate. Since
caches are usually separate from the database, such dedicated caching layers
also introduce additional, complex race conditions. For example, consider a
write-back cache system where a client that misses in the cache reads from the
database and writes back into the cache afterwards. If the data is updated after
the client reads from the database, but before it writes back its (now stale)
result, then the value in cache may now be permanently stale.

Solving this issue, and related issues like ``thundering herds'', requires
highly sophisticated machinery~\cite{facebook-memcache, transactional-cache,
orm-cache, sql-cache} which developers may not even think to employ. A survey
from 2016 found that 0.3-3.0\% of application code spread across 2.1-10.8\% of
the application's source files is caching-related, and that cache-related issues
make up 1-5\% of all issues~\cite{caching-is-hard}. By integrating the cache
with the database, Noria does not suffer the same problems\,---\,the cache is
maintained directly by the database, and does not need to be managed by
application code.

Despite how error-prone the approach is, ad-hoc, per-application caching is
still common in practice. And while Noria eliminates most of the developer
burden of getting caching right, it must offer competitive performance with
manually constructed caching solutions to present a viable alternative.
Unfortunately, this is difficult to evaluate, since high-performance solutions
are often developed specifically for a given application, and not available as
general-purpose tools. And effectively applying the general-purpose tools that
\textbf{are} available, like memcache and redis, requires significant effort on
the part of the application authors (or the evaluators). To manually add caching
support to Lobsters' ~80 queries, including thundering herd mitigation and
incremental updates, would be a massive undertaking.

In an attempt to approximate how a carefully planned and optimized application
caching deployment might perform, the next experiment runs the vote benchmark
against Redis~\cite{redis} with the following modifications:

\begin{itemize}
 \item Every access hits in cache, to emulate perfect thundering herd mitigation
   and invalidation-avoidance schemes.
 \item Nearly all accesses (99.9\%) are reads, since writes would be
   bottlenecked by the backing store.
 \item All accesses are for a single integer value, to emulate a system that has
   perfect cache coverage. Request access keys are chosen according to a Zipfian
    distribution with $\alpha = 1.15$, so that 1\% of keys make up 90\% of
    requests.
 \item Data is not stored anywhere except Redis.
 \item Accesses are batched to reduce serialization cost and increase
   throughput. Specifically, reads are \texttt{MGET}s, and writes are pipelined
    \texttt{INCRBY}s.
\end{itemize}

This is not a realistic use of Redis as a cache, and ignores the complexities of
integrating the cache with the application, but it allows us to ``assume the
best'' about the underlying caching strategy and system. The benchmark runs for
four minutes, and then samples latencies for another two minutes.

\begin{figure}[h]
  \centering
  \includegraphics{graphs/vote-redis.pdf}
  \caption{Achieved throughput vs 90th \%-ile request latency in cache-optimized
  vote. Offered load increases along the points on each line. Vertical
  lines are $16\times$ the cliff of the corresponding experiment.}
  \label{f:vote-redis}
\end{figure}

Figure~\ref{f:vote-redis} shows a throughput-latency plot that explores the
performance profiles of Redis and Noria under these experimental conditions%
\footnote{Noria runs with the same modified access patterns as outlined for
Redis.}. For comparison, it also includes a MySQL + Redis implementation that
stores votes and articles in MySQL, and uses the na\"ive write-back cache
strategy outlined above. Since Redis is not multi-threaded, and so can only use
one of the server's 16 cores, the figure also includes the Redis performance
extrapolated to 16 cores. To achieve this performance in practice, the
application's already-perfect caching scheme would also need to shard perfectly.

Due to the simplifications in this particular benchmark, the experimental
results serve primarily to determine upper bounds for possible performance:
Noria achieves 25\% of the theoretical best you could ever do with Redis. Since
the caching is unlikely to be perfect as in these experiments, Noria's
performance is likely to be close to what a ``real'' caching system achieves.
And Noria does this while providing rich SQL queries without application caching
logic.
