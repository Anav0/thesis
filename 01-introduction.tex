Many of the most popular applications today are content-heavy web applications.
Social networking platforms like Facebook and LinkedIn, content platforms like
Twitter and Wikipedia, communication platforms like Slack and Discord, and
discussion platforms like Reddit and Hacker News all process, display, and
disseminate user-generated content. They share no code, yet they all share an
underlying core feature: they have large amounts of data, and must query that
data every time a user interacts with the site.

These platforms also exhibit many similar traits:
%
\begin{enumerate*}
  \item they are \textbf{interactive}; each page load has a user waiting on the
    other end,
  \item they are \textbf{read-heavy}; most user interactions consume content
    rather than produce new content, and
  \item they experience \textbf{significant skew}; a small number of people,
    posts, creators, teams, and discussions make up the bulk of interactions.
\end{enumerate*}
%
Taken together, these properties suggest that these platforms would benefit
from \emph{caching}\,---\,from a system that ``remembers'' results from past
user requests, and recalls those results efficiently if another user later
requests the same data. Since a user is waiting at the other end of every
request, caching is \emph{necessary} to avoid making the user wait for too long.
Since the platforms are read-heavy, the advantages for reads from caching are
likely to outweight the cost of maintaining the cache. And since access is
skewed, caching even a small fraction of the overall dataset is likely to bring
significant benefits.

Most applications implement caching in an ad-hoc way\,---\,developers introduce
complex and error-prone logic~\cite{ad-hoc-caching} to ensure that changes to
the underlying data are also reflected in results served from the cache. Some
systems, both from industry~\cite{facebook-memcache, tao, flannel} and
academia~\cite{txcache, cachegenie, casql-consistency-thesis, pequod} have
attempted to chip away at this problem, but are often lacking in important ways.
Some require significant developer effort, while others support only a
restricted set of application queries.

A promising avenue for automating at least part of the application caching
problem is \emph{query result caching} through a technique known as
\emph{materialized views}. Materialized views allow an application backend, like
a database, to pre-compute, store, and maintain results for an application's
queries such that subsequent executions of those same queries can be served much
more quickly. Unfortunately, materialized views are typically an all-or-nothing
affair; each query is computed independently, and its full result set is cached.
While this is acceptable for a small number of analytics-type queries, it is a
problem when the application issues a large number of similar, but not identical
queries that must all be answered quickly. Either the common superset of those
queries must be computed in full, which uses significant memory, or each query's
materialized view must be maintained individually, which causes unnecessary
duplicate processing.

This thesis proposes a technique to introduce \textit{partial state} into Noria,
a materialized view system implemented using dataflow~\cite{noria}. Partial
state allows Noria to represent similar queries in a single, shared materialized
view, without simultaneously requiring that the results for \emph{all} such
queries be cached as a result. This let Noria avoid expending memory to cache
query results the application is not interested in, while also allowing similar
queries to share cache maintenance processing.

The work in this thesis proposes to allow individual query results to be marked
as \textit{missing}, and introduces \textit{upqueries} as a mechanism to compute
such missing state on-demand. Upqueries re-use the existing dataflow operators
that maintain the materialized views as the underlying data changes in Noria,
and can thus theoretically be retrofitted onto an existing dataflow system.
Partial state also enables Noria to evict cached results over time as the
application workload changes, which is critical to keep memory use low.

Experiments with partial state in Noria suggest that materialized views
implemented using partial state can increase the supported application load by
up to $20\times$ over MySQL, and can reduce memory use by $\sfrac{2}{3}$
compared to materialized views implemented without partial state.

% Existing materialized view systems also tend to focus on view-update
% performance.

% Commercial materialized views?

% Windowing?

% Replace caching \emph{logic}, not caching \emph{systems}.

\paragraph{Contributions.}

The main contributions of this thesis are:

\begin{itemize}
 \item A model for, and implementation of, partial state in a dataflow-based
   materialized view system.
 \item An algorithm for implementing upqueries to populate missing state on
   demand.
 \item An analysis of the issues that arise when introducing partial state to a
   distributed, high-performance stateful dataflow processing system.
 \item Techniques for overcoming those issues while preserving system
	 correctness, performance, and scalability.
 \item Micro and macro evaluations of the performance and memory impact of
	 introducing partial state to dataflow.
\end{itemize}

\paragraph{Dissertation Outline.}

The rest of the dissertation is organized as follows: Section~\ref{s:noria}
describes the Noria dataflow system that partial state is built on top of.
Section~\ref{s:partial} introduces the partially stateful dataflow model.
Section~\ref{s:correct} describes additional mechanisms that are needed to
ensure that partially stateful dataflow produces correct query results.
Section~\ref{s:eval} evaluates Noria's implementation of partial state on a
realistic application query workload. Section~\ref{s:related} explores related
work. Section~\ref{s:disc} discusses shortcomings of, and alternatives to,
partial state. Finally, Section~\ref{s:future} outlines future work on partial
state.

For readers that are unfamiliar with database queries, materialized views,
dataflow, and application caching, but would still like to understand roughly
what this thesis is about, Appendix~\ref{s:simple} starting on
page~\pageref{s:simple} is for you.

\resume

% Materialized Views

Noria's primary function is to provide materialized views. In database parlance,
a \textit{view} is a name for the result set of a particular query. When an
application reads from a view, it is reading from the result set that results
from executing that query. Most databases also allow views to be used in place
of other database collections, like tables, in other queries.

Some databases can \textit{materialize} a view by storing the result set
somewhere for later recollection. When a view is materialized, queries over that
view do not re-execute the view's backing query, but instead access the rows of
the view as if they were stored in a durable database relation. Materialized
views thus effectively provide query memoization; they are faster to access than
non-materialized views, at the cost of having to store the view's rows.

Once a database materializes a view, it must \textit{maintain} that
materialization over time as the underlying data changes. Otherwise, reads from
the view would no longer match the results of the view's backing query. View
maintenance can be \emph{reactive} by updating the view as the data changes,
\emph{demand-driven} by updating the view only when queried, or \emph{triggered}
by updating the view only when an outside trigger is invoked.

Noria implements reactive view maintenance\,---\,it assumes that writes are less
frequent than reads, and therefore that it is better to make writes slower by
updating the cache proactively than to make reads slower by having each read
check that the materialization is up to date.

\paragraph{Materialization.}
Throughout this thesis, the word materialization is often used as a noun. In the
context of Noria, a materialization refers to any derived computation result
that Noria explicitly stores, not just materialized views. Or, more precisely,
Noria may choose to materialize intermediate results, such as the current value
of an aggregation, which do not represent any of the application's queries.
These intermediate materializations are still views\,---\,they have a schema and
consist of rows\,---\,but do not reflect any named views that the application
has created.

% Caching

As applications grow, their performance needs often exceed that which a
traditional relational database can provide. Such applications cannot afford to
wait for the database to compute joins and aggregations, and need lower-latency
data access than a relational database like MySQL or Postgres typically
provides. In these cases, application authors tend to add \textit{caching} to
their application backend logic.

Caching is a broad topic, especially because caching tends to occur at multiple
levels within the application. Developers may cache results of individual
database queries, collections of data that the application frequently uses
together, computed and otherwise derived values that are expensive to
re-compute, segments of rendered application output (like HTML snippets), all
the way to full web pages. Each of these caches may also exist in multiple
places to improve data locality, add redundancy, or simply because two caches
partially overlap.

Noria implements \textit{query result caching} (using materialized views), which
ensures that the application's database queries are fast, even if they are
complex or include significant computation. While Noria could likely be extended
to maintain the compound caches higher up in the application stack, that is left
for future work.

Much like with view materialization, the introduction of a cache requires that
the application maintains those caches such that new data eventually becomes
visible. The exact mechanism that is used to implement the cache maintenance
varies by cache type, developer preference, and application size.

Once a cache is in place, the application authors must also implement cache
\textit{eviction} to ensure that the cache does not grow without bound. Many
eviction strategies exist, and revolve around the notion that certain cache
entries are more worthwhile to keep than others. These entries are usually
referred to as ``hot'' entries. Essentially, if storage space is limited, it is
likely better to keep hot a cache entry that is accessed once a second than a
``cold'' entry that was last accessed two weeks ago. \textit{Partial state}, the
topic of this thesis, is what allows Noria to implement eviction for
materialized views.

% Dataflow
% Frans likes this?

Noria maintains its materialized views using \textit{dataflow}. Dataflow means a
thousand things to a thousand people, especially if they are from different
areas of computer science, but this thesis will focus on dataflow as understood
in software computing specifically. Even there, many definitions exist, but they
all agree that in dataflow, compute is stationary while data moves.

A dataflow system expresses complex computations as a directed \emph{graph} that
consists of interconnected nodes that implement primitive operations, such as
joins, filters, and aggregations. These nodes then \emph{stream} data among each
other to produce the desired overall results. Dataflow systems are thus also
often referred to as stream processing systems. The roots of the graph introduce
the data that the dataflow computes over, while the leaves represent the
dataflow program's outputs.

Since the dataflow model is inherently streaming, it is particularly well-suited
for distributed deployments. Nodes are independent and communicate only through
their streams, so the system can easily place them on different cores, or hosts,
and use existing messaging fabrics to connect them. If needed, nodes can also be
sharded to allow data-parallel computing.

The streaming design also makes dataflow a good fit for reactive applications.
When the data changes, the system introduces the changes at the roots, and then
observe how the leaves change in response. Those changes can in turn be
forwarded to the application to provide a change stream over the computation's
results over time.
