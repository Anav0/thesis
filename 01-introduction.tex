This dissertation proposes a practical database system that lowers latency and
increases supported load for read-heavy web applications. Where traditional
databases re-execute queries each time they are issued, the presented system
stores and maintains a cache of past query results, and uses that to serve
queries efficiently. As opposed to existing state-of-the-art systems, the
presented system supports building the cache on demand, and is able to evict
cache entries in response to a shifting workload.

Experimental results suggest that the presented system increases supported
application load by up to $20\times$ over MySQL, and reduces memory use by up to
$\sfrac{2}{3}$ compared to existing approaches.

\section{Motivation}

Modern web applications typically have a number of traits in common. They are
\textbf{interactive}: each incoming request has a user waiting on the other end.
They are \textbf{read-heavy}: most interactions consume content rather than
produce new content. And they experience \textbf{significant skew}: a small
number of people, posts, teams, and discussions make up the bulk of
interactions.

Such applications are usually poorly served by the traditional relational
databases that most of them use to store and query their underlying data.
These application tend to issue the same read-only queries again and again, with
the underlying data changing only infrequently. Existing databases do
not optimize for this kind of workload: they run each query in isolation, and
thus re-do work that has already been done many times over.

As a result, application authors often resort to ad-hoc, error-prone
techniques~\cite{ad-hoc-caching} to exploit their applications' workload
patterns and satisfy impatient users. They de-normalize their database schemas
by placing and updating computed values in the database tables, or introduce
key-value stores that \textit{cache} the result of expensive queries. All these
techniques introduce significant application complexity: the application authors
must include logic to ensure that the auxiliary derived state remains up to date
as the underlying data changes, that clients do not all flood the database when
results are not available in the cache, and that concurrent access to the
database and the cache never leaves the system in an inconsistent state.

\section{Existing Solutions}

Existing systems from industry~\cite{facebook-memcache, tao, flannel} and
academia~\cite{txcache, cachegenie, casql-consistency-thesis, pequod} have
attempted to chip away at this problem, but are often lacking in important ways.
Some require significant developer effort, and are infeasible to implement for
any but the largest companies. Some support only a restricted set of application
queries, or only provide infrastructure for developers to implement caching
themselves. Many maintain cached results only by evicting old results, and
cannot updating existing results in-place, which is wasteful.

% Noria replaces caching \emph{logic}, not caching just caching \emph{systems}.

Eons ago, the database community produced \textit{materialized views} as a
potential answer to the problem of slow queries. Materialized views store the
results of \textit{views} (i.e., named queries) with the goal of making those
queries faster to execute~\cite{materialized-views}. These materialized views
can then be maintained incrementally rather than re-executing queries when the
underlying data changes~\cite{materialized-survey}. However, few commercially
available databases support materialized views, and the ones that do come with
significant restrictions~\cite{mssql-materialized-view-restrictions}.

State-of-the-art research systems support flexible materialized
views~\cite{dbtoaster,materialize}, but do not support eviction or low-latency
reads\footnote{In these systems, reads cannot access the materialized view
directly, and must synchronize with the write-processing pipeline to extract
query results.}, and thus function poorly as a replacement for a cache. Many
recent state-of-the-art materialized view systems are also restricted to a
pre-declared set of queries, and cannot incorporate changes to the application
query set without restarting.

\section{Approach: Partial State}

Materialized views represent an ``almost there'' solution to automatic caching.
They provide a great foundational mechanism for storing and maintaining query
results efficiently in a way that meshes well with how applications already
work: by issuing SQL queries. What is missing to make materialized views a
viable replacement for the ad-hoc caching strategies today's applications employ
is a way to make the materialized views more dynamic. Specifically, to serve as
a good cache-substitute, materialized views must support efficiently adding new
queries and evicting old results at runtime.

To bridge the gap, this thesis proposes \textit{partially materialized state},
or partial state for short. Partial state lets entries in materialized views be
marked as \textit{missing}, and introduces \textit{upqueries} to compute such
missing state on-demand. This allows new queries to be added efficiently by
leaving the initial materialized view empty, and populating the view only in
response to application queries. Furthermore, as the application loses interest
in old query results, those results can be evicted to reclaim memory, which can
in turn be used to cache more important query results. In essence, partial state
enables materialized views to function like caches.

The thesis includes an implementation of partial state in Noria, a
state-of-the-art materialized view system that is already optimized for
read-heavy, dynamic web applications~\cite{noria}. Noria uses \textit{dataflow}
internally to maintain its materialized views, a system architecture that allows
fast and distributed computation over a stream of data changes. Dataflow
represents computational dependencies as a directed acyclic graph where edges
represent data dependencies, and vertices represent computations (like
aggregations or joins) over the data that arrives over the incoming edges.
Partial state upqueries flow ``up'' this dataflow, in the opposite direction of
the data, to replay past state in the case of a cache miss. Those replays then
use the existing Noria dataflow to process the responses and fill in missing
state. This avoids the need for separate logic for serving cache misses and
maintaining already cached state, and simplifies the implementation.

\section{Contributions}

The main contributions of this thesis are:

\begin{itemize}
 \item A model for, and implementation of, partial state in a dataflow-based
   materialized view system.
 \item An algorithm for implementing upqueries to populate missing state on
   demand.
 \item An analysis of the issues that arise when introducing partial state to a
   distributed, high-performance stateful dataflow processing system.
 \item Techniques for overcoming those issues while preserving system
	 correctness, performance, and scalability.
 \item Micro and macro evaluations of the performance and memory impact of
	 introducing partial state to dataflow.
\end{itemize}

\section{Reading Guide}

The rest of the dissertation is organized as follows: Chapter~\ref{s:noria}
describes the Noria dataflow system. Chapter~\ref{s:partial} introduces the
partially stateful dataflow model. Chapter~\ref{s:correct} describes additional
mechanisms that are needed to ensure that partially stateful dataflow produces
correct query results. Chapter~\ref{s:eval} evaluates Noria's implementation of
partial state on a realistic application query workload. Chapter~\ref{s:related}
explores related work. Chapter~\ref{s:disc} discusses shortcomings of, and
alternatives to, partial state. Finally, Chapter~\ref{s:future} outlines future
work on partial state.

For readers that are unfamiliar with database queries, materialized views,
dataflow, and application caching, but would still like to understand roughly
what this thesis is about, Appendix~\ref{s:simple} starting on
page~\pageref{s:simple} is for you.
